{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4907a9b-7f61-4e03-a118-3ca33156b516",
   "metadata": {},
   "source": [
    "# Ethics, Fairness and Explanation in AI Coursework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18977034-9183-47c7-993e-9b105ebaf8bc",
   "metadata": {},
   "source": [
    "Your goal in this coursework is to implement and experiment with various explainability approaches in order to better understand the behaviour of a neural model applied to the Titanic dataset. As you will have a chance to observe, the dataset reflects some of the past social conventions and biases, which also affect the trained model. Explanations can serve as very useful tools for identifying such potential issues and gaining insight into the internal reasoning of machine learning systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c54e2fe-cc8f-4b0d-852a-474e43b5b060",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f683e86-6c7d-4ce1-9cc9-a3ec171824d8",
   "metadata": {},
   "source": [
    "We start by defining some helpful utility functions for data preprocessing. You will probably not need to change this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "e3853c0d-4ab5-41dc-a409-ef3b7dab0911",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import RobustScaler, OneHotEncoder\n",
    "from sklearn.utils import resample\n",
    "\n",
    "\n",
    "class InvertibleColumnTransformer(ColumnTransformer):\n",
    "    \"\"\"\n",
    "    This is an invertible version of a ColumnTransformer from sklearn.\n",
    "    This allows us to recover the original feature values from their normalised\n",
    "    versions in order to better understand the produced explanations.\n",
    "    \"\"\"\n",
    "    def inverse_transform(self, X):\n",
    "        if X.ndim == 1:\n",
    "            X = np.expand_dims(X, axis=0)\n",
    "        if X.shape[1] != len(self.get_feature_names_out()):\n",
    "            raise ValueError(\n",
    "                \"X and the fitted transformer have different numbers of columns\"\n",
    "            )\n",
    "\n",
    "        inverted_X_base = np.zeros((X.shape[0], self.n_features_in_))\n",
    "        columns = [c for cs in self._columns for c in cs]\n",
    "        inverted_X = pd.DataFrame(data=inverted_X_base, columns=columns)\n",
    "        inverted_X = inverted_X.astype('object')\n",
    "        for name, indices in self.output_indices_.items():\n",
    "            transformer = self.named_transformers_.get(name, None)\n",
    "            if transformer is None:\n",
    "                continue\n",
    "\n",
    "            selected_X = X[:, indices.start : indices.stop]\n",
    "            if isinstance(transformer, OneHotEncoder):\n",
    "                # Assumed only one column changing encoder at the end\n",
    "                categories = transformer.inverse_transform(selected_X)\n",
    "                inverted_X.loc[\n",
    "                    :, columns[indices.start : indices.start + len(categories[0])]\n",
    "                ] = categories\n",
    "            else:\n",
    "                # Assumed scaler-type transformer\n",
    "                inverted_X.loc[\n",
    "                    :, [columns[i] for i in range(indices.start, indices.stop)]\n",
    "                ] = transformer.inverse_transform(selected_X)\n",
    "\n",
    "        return inverted_X\n",
    "\n",
    "\n",
    "def preprocess_train_data(\n",
    "    df,\n",
    "    scaled_features=None,\n",
    "    categorical_features=None,\n",
    "    scaler=RobustScaler(quantile_range=(10, 90)),\n",
    "    categorical_encoder=OneHotEncoder(handle_unknown=\"ignore\"),\n",
    "):\n",
    "    \"\"\"\n",
    "    Scales the continuous features using a RobustScaler and one-hot encodes\n",
    "    the categorical features.\n",
    "    \"\"\"\n",
    "    if scaled_features is None and categorical_features is None:\n",
    "        warnings.warn(\"No features specified for preprocessing, using raw data.\")\n",
    "        scaled_features = []\n",
    "        categorical_features = []\n",
    "    elif scaled_features is None:\n",
    "        scaled_features = [c for c in df.columns if c not in categorical_features]\n",
    "    elif categorical_features is None:\n",
    "        categorical_features = [c for c in df.columns if c not in scaled_features]\n",
    "\n",
    "    preprocessor = InvertibleColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", scaler, scaled_features),\n",
    "            (\"cat\", categorical_encoder, categorical_features),\n",
    "        ],\n",
    "        remainder=\"passthrough\",\n",
    "    )\n",
    "\n",
    "    preprocessed_df = preprocessor.fit_transform(df)\n",
    "    return preprocessed_df, preprocessor\n",
    "\n",
    "\n",
    "def preprocess_test_data(df, preprocessor):\n",
    "    preprocessed_df = preprocessor.transform(df)\n",
    "    return preprocessed_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df35c3a2-09c0-4a14-9f83-e81db1d544a1",
   "metadata": {},
   "source": [
    "Here, we define a class for the Titanic dataset, which we will be using throughout the coursework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "22f7f44d-1cbe-4e25-9f7c-7bbe8c2d4cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class TitanicDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Loads and preprocesses the Titanic dataset.\n",
    "    \"\"\"\n",
    "    __create_key = object()\n",
    "\n",
    "    @classmethod\n",
    "    def create_datasets(\n",
    "        cls,\n",
    "        label_name=\"survived\",\n",
    "        split_seed=42,\n",
    "        test_size=0.2,\n",
    "    ):\n",
    "        train_dataset = TitanicDataset(\n",
    "            cls.__create_key,\n",
    "            label_name=label_name,\n",
    "            split_seed=split_seed,\n",
    "            test_size=test_size,\n",
    "            train=True,\n",
    "        )\n",
    "        test_dataset = TitanicDataset(\n",
    "            cls.__create_key,\n",
    "            label_name=label_name,\n",
    "            split_seed=split_seed,\n",
    "            test_size=test_size,\n",
    "            train=False,\n",
    "        )\n",
    "        return train_dataset, test_dataset\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        create_key=None,\n",
    "        label_name=\"Survived\",\n",
    "        split_seed=42,\n",
    "        test_size=0.2,\n",
    "        train=True,\n",
    "    ):\n",
    "        # Ensure that the dataset is being constructed properly\n",
    "        if create_key != TitanicDataset.__create_key:\n",
    "            raise ValueError(\n",
    "                \"Illegal initialisation attempt â€” please use create_datasets to initialise.\"\n",
    "            )\n",
    "\n",
    "        try:\n",
    "            data_df = pd.read_csv(\"titanic-dataset.csv\")\n",
    "        except FileNotFoundError:\n",
    "            raise FileNotFoundError(\"Titanic data file not found.\")\n",
    "\n",
    "        # Split the dataset into train and test\n",
    "        x = data_df.drop(columns=[label_name, \"name\", \"ticket\", \"cabin\", \"embarked\", \"boat\", \"body\", \"home.dest\"])\n",
    "        # For the purposes of this coursework, we just impute the missing age and fare with a median value\n",
    "        x[['age']] = x[['age']].fillna(x[['age']].median())\n",
    "        x[['fare']] = x[['fare']].fillna(x[['fare']].median())\n",
    "        y = data_df[label_name]\n",
    "        x_train, x_test, y_train, y_test = train_test_split(\n",
    "            x, y, test_size=test_size, random_state=split_seed, shuffle=True\n",
    "        )\n",
    "        if train:\n",
    "            self.raw_data = x_train, y_train\n",
    "        else:\n",
    "            self.raw_data = x_test, y_test\n",
    "\n",
    "        # Preprocess the data\n",
    "        x_train_processed, preprocessor = preprocess_train_data(\n",
    "            x_train, categorical_features=[\"sex\"]\n",
    "        )\n",
    "        x_train = pd.DataFrame(\n",
    "            x_train_processed, columns=preprocessor.get_feature_names_out()\n",
    "        )\n",
    "        x_test_processed = preprocess_test_data(x_test, preprocessor)\n",
    "        x_test = pd.DataFrame(\n",
    "            x_test_processed, columns=preprocessor.get_feature_names_out()\n",
    "        )\n",
    "\n",
    "        # Select data partition and convert to tensors\n",
    "        if train:\n",
    "            samples = x_train\n",
    "            labels = y_train\n",
    "        else:\n",
    "            samples = x_test\n",
    "            labels = y_test\n",
    "        self.samples = torch.tensor(samples.to_numpy(), dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels.to_numpy(), dtype=torch.long)\n",
    "        self.features = preprocessor.get_feature_names_out()\n",
    "        self.preprocessor = preprocessor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00c7c1f-81f5-4806-ad20-358a79df93e6",
   "metadata": {},
   "source": [
    "Finally, we call the code above to load and preprocess the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "2ae2d1e0-ddab-419b-b741-6d4d6b531bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = TitanicDataset.create_datasets(\n",
    "    test_size=0.2,\n",
    "    split_seed=42,\n",
    ")\n",
    "train_dl = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=128,\n",
    "    shuffle=False,\n",
    ")\n",
    "test_dl = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4283a2e-986b-4554-89e3-3a0c4c152cbb",
   "metadata": {},
   "source": [
    "Note that the invertible transformer allows you to recover the original (unnormalised) feature values, as shown on the example below. You may find this helpful for understanding the produced explanations and commenting on them in your report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "88f85249-8c4b-46b0-a856-76c972f524e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pclass</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>fare</th>\n",
       "      <th>sex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.125</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  pclass   age sibsp parch   fare   sex\n",
       "0    3.0  35.0   0.0   0.0  7.125  male"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset.preprocessor.inverse_transform(test_dataset.samples[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441756a7-f828-4764-af69-4eb84b63a975",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93538903-f8cc-4f39-9593-f0ee82096b75",
   "metadata": {},
   "source": [
    "When faced with a new dataset, it is a good practice to perform an exploratory data analysis in order to understand the basic trends in the data. This will also allow you to put the explanations you obtain as part of this coursework into the relevant context. We will use the raw, unnormalised features for this purpose, as they are much more intuitive and human-understandable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "a82a5efb-2ce7-4428-a03f-ee0f61f6dca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = train_dataset.raw_data\n",
    "x_train['survived'] = y_train\n",
    "data_df = x_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf759d8-e225-4482-a8ba-93d0a604d32e",
   "metadata": {},
   "source": [
    "We start by displaying the feature values and labels for a few samples. The dataset contains data regarding the survival of some of the passengers involved in the [Titanic maritime disaster](https://en.wikipedia.org/wiki/Sinking_of_the_Titanic). The features contained in the data are as follows:\n",
    "* `pclass`: Indicates the travelling class of the given passenger. Note that we treat this feature as numerical, as the different classes introduce a natural order.\n",
    "* `sex`: Indicates the sex of the passenger.\n",
    "* `age`: Provides the age of the passenger.\n",
    "* `sibsp`: Denotes the total number of siblings and spouses of the given passenger also travelling on RMS Titanic.\n",
    "* `parch`: Denotes the total number of parents or children of the given passenger also travelling on RMS Titanic.\n",
    "* `fare`: Indicates the fare paid by the passenger for the journey.\n",
    "* `survived`: The label indicating whether the patient survived the accident (1 = survived, 0 = did not survive).\n",
    "\n",
    "There are other features included in the original dataset (such aspassenger name or point of embarkation), but we choose to ignore them for the purposes of this coursework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "9bc9b42a-9f57-4502-8d00-9fda6bdb34cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pclass</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>fare</th>\n",
       "      <th>survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>772</th>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.8958</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>543</th>\n",
       "      <td>2</td>\n",
       "      <td>male</td>\n",
       "      <td>36.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10.5000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>79.6500</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>47.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>227.5250</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>42.4000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1095</th>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.6292</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1130</th>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.7750</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1294</th>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>28.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16.1000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1126</th>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.8958</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1047 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      pclass     sex   age  sibsp  parch      fare  survived\n",
       "772        3    male  17.0      0      0    7.8958         0\n",
       "543        2    male  36.0      0      0   10.5000         0\n",
       "289        1  female  18.0      0      2   79.6500         1\n",
       "10         1    male  47.0      1      0  227.5250         0\n",
       "147        1    male  28.0      0      0   42.4000         0\n",
       "...      ...     ...   ...    ...    ...       ...       ...\n",
       "1095       3  female  28.0      0      0    7.6292         0\n",
       "1130       3  female  18.0      0      0    7.7750         0\n",
       "1294       3    male  28.5      0      0   16.1000         0\n",
       "860        3  female  26.0      0      0    7.9250         1\n",
       "1126       3  female  28.0      0      0    7.8958         0\n",
       "\n",
       "[1047 rows x 7 columns]"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d8e9c2-2266-40c2-9d91-3bd25e633eb2",
   "metadata": {},
   "source": [
    "Let us visualise the correlation between the individual columns of the data, computed using the [Pearson correlation coefficient](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient). Note that we excluded the `sex` feature from this visualisation, as it is categorical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "465f7fb2-3f81-4f9a-9ff9-09f45d3a50e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "corr = data_df.drop(columns=[\"sex\"]).corr()\n",
    "sns.heatmap(\n",
    "    corr,\n",
    "    xticklabels=corr.columns.values,\n",
    "    yticklabels=corr.columns.values,\n",
    "    annot=True,\n",
    "    fmt='.2g',\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba12652-00ee-42f2-b37c-7aabbd7d6b17",
   "metadata": {},
   "source": [
    "Since the previous plot does not include the categorical `sex` column, we also separately visualise its distribution, including the associated labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "93e255d4-c9d4-4588-aeb3-f563c202b595",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjsAAAHSCAYAAAAKdQqMAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAx/UlEQVR4nO3dCXhURbr/8TchIewJ+zLsCkJkk0XWYS4QiGyCAQUFZJTBGQRkB6MIClwCGVkGFDOiLCoMiohXcUAgrEJYveyLoChoIGHAJICSAMn/eWv+3TcNQXFIcror38/znKfT55ykqzNk+mfVW1V+GRkZGQIAAGApf6cbAAAAkJMIOwAAwGqEHQAAYDXCDgAAsBphBwAAWI2wAwAArEbYAQAAVgtwugHeID09XeLj46Vo0aLi5+fndHMAAMAd0KUCL126JBUqVBB//9v33xB2REzQqVSpktPNAAAA/4EzZ85IxYoVb3udsCNienRcv6xixYo53RwAAHAHUlJSTGeF63PcK8NO1apV5bvvvrvl/LPPPiuvv/66XL16VUaNGiXLli2T1NRUCQ8Pl3nz5knZsmXd954+fVoGDRokGzdulCJFikj//v0lKipKAgLu/K25hq406BB2AADwLb9WguJogfLu3bvl7Nmz7mPdunXm/KOPPmoeR4wYIZ9++qksX75cNm/ebIabIiIi3N9/48YN6dy5s6Slpcn27dtl8eLFsmjRIpkwYYJj7wkAAHgXP2/aCHT48OGyatUqOXHihOmaKl26tCxdulR69uxprh87dkxq164tcXFx0qxZM1m9erV06dLFhCBXb09MTIyMGzdOzp8/L/nz57+j19XXCg4OluTkZHp2AADwEXf6+e01U8+1d+a9996Tp59+2nRH7d27V65duyZhYWHue2rVqiWVK1c2YUfpY926dT2GtXSoS9/84cOHb/taOiSm92Q+AACAnbymQPnjjz+WpKQk+eMf/2ienzt3zvTMhISEeNynwUavue7JHHRc113Xbkdrel555ZXfPD1dAxn+T2BgoOTLl8/pZgAA4Bth5+2335aOHTuaufI5LTIyUkaOHHlLNfftaMg5deqUCTzwpGG0XLlyrE8EAPBaXhF2dEbW+vXr5aOPPnKf0w9QDRna25O5dychIcFcc92za9cuj5+l113XbicoKMgcd0JLmrR4WnswNBD90qJFeYn+Xn766SdJTEw0z8uXL+90kwAA8N6ws3DhQilTpoyZWeXSqFEjM0wSGxsrPXr0MOeOHz9uppo3b97cPNfH//7v/zYfuPr9Smd0aZFSaGhotrTt+vXr5kNde5wKFSqULT/TFgULFjSPrt8/Q1oAAG/keNjRoSENO7o+Tua1cbS6esCAAWa4qUSJEibADB061AQcnYmlOnToYEJNv379JDo62tTpjB8/XgYPHnzHPTe/Rqe3qzud2ZXXuAKgFpMTdgAA3sjxsKPDV9pbo7OwbjZr1iwzbKQ9O5kXFXTRD1edqq6LCmoIKly4sAlNkyZNyvZ2UpOSNX4vAABv51Xr7HjjPH1dxVmLk6tVqyYFChRwrI3eit8PAMApPrfODgAAQE4g7PioTZs2mSEkna2Wk3Tdo+7du+foawAAkJMIO3dJt6XQmiFd2VmLonXKu9YWbdu2LUdft0WLFmZKvHbfAQAALy5Q9nVaPK3rAekmpNWrVzfr/Oh0+QsXLvxHP09LqHQG2K/t2q6zw35pLSEAAPBv9OzcBR1C2rp1q0yfPl3atGkjVapUkQcffNCs0Pzwww/Lt99+a4aa9u3b5/E9ek6HoTIPR+mmprq2kPYOLViwwJzTjU9vnp12zz33eHyf/jwt0NI1b/RnZLZy5UopWrSoWSdInTlzRh577DGzSKNO5+/WrZtpo4uGLJ3qr9dLliwpY8eONeELAABfRs/OXShSpIg5dF8vXfvnbtb2ef755+XVV181vUPFixeX+fPny5IlS2Ty5Mnue/T5E088ccv3agW67v6uO8TrlhuZ79d6G10LR9fB0eE1naKvAU17jqZMmSIPPfSQHDhwwPQUzZgxQxYtWmTClu4ur881MLVt2/Y/fl8AnJMvupXTTUAuujH2C6eb4LXo2bkLGhg0HOgQlvaGtGzZUl544QUTHn4rXRuoffv2pudGe1369Okj//jHP9zXv/rqK7MTvJ7Pip7X0OXqxdHens8++8x9//vvv28WcHzrrbfMTvEaZnQxR13jyNXLNHv2bNMrFRERYa7HxMRQEwQA8HmEnWyo2YmPj5dPPvnE9JJocGjYsKEJQb9F48aNPZ737t3bDDHt2LHD3UujP7dWrVpZfn+nTp3M9hraDrVixQrT4xMWFmae79+/X06ePGmGtVw9UhqqdJ2cr7/+2qxRoAXPTZs29QhzN7cLAABfQ9jJBrqYnvbKvPTSS7J9+3YzXXvixInuTUMz173ocFJWdPXnzLT4WIePdGhK6ePtenWUDkP17NnT4/5evXq5C50vX75saoK0fijzoT1GWQ2NAQBgC8JODtD9uq5cuSKlS5c2z7XHxCVzsfKv0XCjw09xcXHyzTffmN6eX7t/zZo1cvjwYdmwYYNHONJeoRMnTpgNO++9916PQ4eq9NCdy3fu3OmxCaoOnQEA4MsIO3dBp5dr78t7771n6nR024Tly5ebTUl1ppPOkNLC5WnTpsnRo0dl8+bNZqPSO6W1M5cuXTLr+OhsL915/Ze0bt3a9AhpyNHtGzIPSem5UqVKmXZpgbK2VYfcnnvuOfn+++/NPcOGDTNt1dofnQn27LPP5viihQAA5DTCzl3QuhcNFDolXINGnTp1zFDWwIED5bXXXjP36Mwm7SHRIaThw4ebGVB3SutrunbtauptfmkIy0Wnoj/++ONZ3q8zsrZs2WIWP3QVIOuu8lqz49pPZNSoUWYHed1MVWdt6es/8sgjv/n3AgCAN2EjUDYCvSv8fgDvxdTzvCUvTj1PYSNQAAAAwg4AALAcYQcAAFiNsAMAAKxG2AEAAFYj7AAAAKsRdgAAgNUIOwAAwGqEHQAAYLV/b4mN32xpBb9cfb0n4n/bQte68/rixYslKipKnn/+efd53fdKt4Bg4WwAQF5Bz47FdPuG6dOny48//uh0UwAAcAxhx2JhYWFmF3Tt3bmdFStWyP333y9BQUFStWpVmTFjRq62EQCAnEbYsVi+fPlk6tSpMnfuXPn+++9vub5371557LHHpHfv3nLw4EF5+eWXza7tixYtcqS9AADkBMKO5bQ+p0GDBjJx4sRbrs2cOVPatWtnAk7NmjVNnc+QIUPkr3/9qyNtBQAgJxB28gCt29Fi5aNHj3qc1+ctW7b0OKfPT5w4ITdu3MjlVgIAkDMIO3lA69atJTw8XCIjI51uCgAAuY6p53nEtGnTzHDWfffd5z5Xu3Zt2bZtm8d9+lyHtLTeBwAAGxB28oi6detKnz59ZM6cOe5zo0aNkiZNmsjkyZOlV69eEhcXJ6+99prMmzfP0bYCAJCdGMbKQyZNmiTp6enu5w0bNpQPPvhAli1bJnXq1JEJEyaYe7RQGQAAW9Czk0srGue2rKaP6zo6qampHud69OhhDgAAbEXPDgAAsBphBwAAWI2wAwAArEbYAQAAViPsAAAAqxF2AACA1Qg7AADAaoQdAABgNcfDzg8//CB9+/aVkiVLSsGCBc22Bnv27HFfz8jIMCv7li9f3lwPCwszu3JndvHiRbMVQrFixSQkJEQGDBggly9fduDdAAAAb+No2Pnxxx+lZcuWEhgYKKtXr5YjR47IjBkzpHjx4u57oqOjzX5OMTExsnPnTilcuLDZwfvq1avuezToHD58WNatWyerVq2SLVu2yDPPPOPQuwIAAN7E0e0ipk+fLpUqVZKFCxe6z1WrVs2jV2f27Nkyfvx46datmzn3zjvvSNmyZeXjjz+W3r17y9GjR2XNmjWye/duady4sbln7ty50qlTJ3n11VelQoUKOdL2fNGtJDfdGPvFHd+rv7f27dubncs///xzj2u6yecLL7wghw4dkooVK+ZASwEA8C6O9ux88sknJqA8+uijUqZMGXnggQdk/vz57uunTp2Sc+fOmaErl+DgYGnatKnZoVvpow5duYKO0vv9/f1NT1BWdH+olJQUj8Mmfn5+JkDq+//73//u8fscO3asCYMEHQBAXuFo2Pnmm2/kjTfekBo1apgeiEGDBslzzz0nixcvNtc16CjtyclMn7uu6aMGpcwCAgKkRIkS7ntuFhUVZUKT69DeJdvoe/rb3/4mo0ePNiFHe3u0lqlDhw4mVHbs2FGKFClifpf9+vWTf/3rX+7v/fDDD03tlNZIaS2VhscrV644+n4AAPDJsJOeni4NGzaUqVOnmg9grbMZOHCgqc/JSZGRkZKcnOw+zpw5Izbq37+/tGvXTp5++ml57bXXzNCV9vS0bdvW/L61EFyHABMSEuSxxx4z33P27Fl5/PHHzffoEOGmTZskIiLChCUAAHyRozU7OsMqNDTU41zt2rVlxYoV5uty5cqZR/0w1ntd9HmDBg3c9yQmJnr8jOvXr5sZWq7vv1lQUJA58oI333xT7r//flO0rb9XDTsadDRguixYsMD0BH311VdmFpv+/jTgVKlSxVzXXh4AAHyVoz07OhPr+PHjHuf0A9f1IavFyhpYYmNj3de1vkZrUZo3b26e62NSUpLs3bvXfc+GDRtMr5HW9uR1OsT35z//2YTI7t27y/79+2Xjxo1mCMt11KpVy9z79ddfS/369U1vkAYcraXSGiqdNQcAgK9ytGdnxIgR0qJFC9PLoMMou3btMj0RergKbYcPHy5TpkwxdT0afl566SUzw0o/uJV+iD/00EPu4a9r167JkCFDzEytnJqJ5Wu0hkkPpT03Xbt2NTPhbqa9ZzqDS6fwb9++XdauXWuKmV988UUTMDPPlAMAwFc4GnaaNGkiK1euNDU0kyZNMh+mOtVc181x0dlDWhyr9Tzag9OqVStTZ1KgQAH3PUuWLDEBR3skdBZWjx49zNo8uJXWSOlwVtWqVd0B6GYaMrXXTQ9d0FF72vR/p5EjR+Z6ewEA8Omwo7p06WKO29EPXg1CetyOzrxaunRpDrXQLoMHDzZDU1qErEFSf3cnT56UZcuWyVtvvWWKlnXYUGdt6RCY9uicP3/e9KABAOCLHA87yF06tLdt2zYZN26cCTS65pD23OhQoPaK6ZYbWsysPWxaH6XXdFVrnaoOAIAv8stgTrH5UNf1dnQaun7YZ6bbUug6NTrElnnoDP/G7wfwXrm90juc9VtW2s8Ln99etREoAABATiLsAAAAqxF2AACA1Qg7AADAaoSdO0Qdd9b4vQAAvB1h51foisIqLS3N6aZ4pZ9++sk8BgYGOt0UAACyxDo7v0JXGS5UqJBZWE8/0HUtGvy7R0eDjm7CGhIS4g6FAAB4G8LOr9AVnHXPKF1L5rvvvnO6OV5Hg87tdpcHAMAbEHbuQP78+c1GpAxledKeLnp0AADejrBzh3T4ihWCAQDwPRSgAAAAqxF2AACA1RjGAgBLvTt7m9NNQG4a63QDvBc9OwAAwGqEHQAAYDXCDgAAsBphBwAAWI2wAwAArEbYAQAAViPsAAAAqxF2AACA1Qg7AADAaoQdAABgNcIOAACwGmEHAABYjbADAACsRtgBAABWI+wAAACrEXYAAIDVCDsAAMBqhB0AAGA1wg4AALAaYQcAAFiNsAMAAKxG2AEAAFYj7AAAAKsRdgAAgNUIOwAAwGqEHQAAYDVHw87LL78sfn5+HketWrXc169evSqDBw+WkiVLSpEiRaRHjx6SkJDg8TNOnz4tnTt3lkKFCkmZMmVkzJgxcv36dQfeDQAA8EYBTjfg/vvvl/Xr17ufBwT8X5NGjBghn332mSxfvlyCg4NlyJAhEhERIdu2bTPXb9y4YYJOuXLlZPv27XL27Fl58sknJTAwUKZOnerI+wEAAN7F8bCj4UbDys2Sk5Pl7bfflqVLl0rbtm3NuYULF0rt2rVlx44d0qxZM1m7dq0cOXLEhKWyZctKgwYNZPLkyTJu3DjTa5Q/f34H3hEAAPAmjtfsnDhxQipUqCDVq1eXPn36mGEptXfvXrl27ZqEhYW579UhrsqVK0tcXJx5ro9169Y1QcclPDxcUlJS5PDhw7d9zdTUVHNP5gMAANjJ0bDTtGlTWbRokaxZs0beeOMNOXXqlPz+97+XS5cuyblz50zPTEhIiMf3aLDRa0ofMwcd13XXtduJiooyw2Kuo1KlSjny/gAAQB4fxurYsaP763r16pnwU6VKFfnggw+kYMGCOfa6kZGRMnLkSPdz7dkh8AAAYCfHh7Ey016cmjVrysmTJ00dT1pamiQlJXnco7OxXDU++njz7CzX86zqgFyCgoKkWLFiHgcAALCTV4Wdy5cvy9dffy3ly5eXRo0amVlVsbGx7uvHjx83NT3Nmzc3z/Xx4MGDkpiY6L5n3bp1JryEhoY68h4AAIB3cXQYa/To0dK1a1czdBUfHy8TJ06UfPnyyeOPP25qaQYMGGCGm0qUKGECzNChQ03A0ZlYqkOHDibU9OvXT6Kjo02dzvjx483aPNp7AwAA4GjY+f77702wuXDhgpQuXVpatWplppXr12rWrFni7+9vFhPUGVQ602revHnu79dgtGrVKhk0aJAJQYULF5b+/fvLpEmTHHxXAADAm/hlZGRkSB6nBcrak6Rr+1C/A8AWSyv4Od0E5KIn4vPex3nKHX5+e1XNDgAAQHYj7AAAAKsRdgAAgNUIOwAAwGqEHQAAYDXCDgAAsBphBwAAWI2wAwAArEbYAQAAViPsAAAAqxF2AACA1Qg7AADAaoQdAABgNcIOAACwGmEHAABYjbADAACsRtgBAABWI+wAAACrEXYAAIDVCDsAAMBqhB0AAGA1wg4AALAaYQcAAFiNsAMAAKxG2AEAAFYj7AAAAKsRdgAAgNUIOwAAwGqEHQAAYDXCDgAAsBphBwAAWI2wAwAArEbYAQAAViPsAAAAqxF2AACA1Qg7AADAaoQdAABgNcIOAACwGmEHAABYjbADAACsRtgBAABW85qwM23aNPHz85Phw4e7z129elUGDx4sJUuWlCJFikiPHj0kISHB4/tOnz4tnTt3lkKFCkmZMmVkzJgxcv36dQfeAQAA8EZeEXZ2794tf//736VevXoe50eMGCGffvqpLF++XDZv3izx8fESERHhvn7jxg0TdNLS0mT79u2yePFiWbRokUyYMMGBdwEAALyR42Hn8uXL0qdPH5k/f74UL17cfT45OVnefvttmTlzprRt21YaNWokCxcuNKFmx44d5p61a9fKkSNH5L333pMGDRpIx44dZfLkyfL666+bAAQAAOB42NFhKu2dCQsL8zi/d+9euXbtmsf5WrVqSeXKlSUuLs4818e6detK2bJl3feEh4dLSkqKHD58+LavmZqaau7JfAAAADsFOPniy5Ytky+//NIMY93s3Llzkj9/fgkJCfE4r8FGr7nuyRx0XNdd124nKipKXnnllWx6FwAAwJs51rNz5swZGTZsmCxZskQKFCiQq68dGRlphslch7YFAADYybGwo8NUiYmJ0rBhQwkICDCHFiHPmTPHfK09NFp3k5SU5PF9OhurXLly5mt9vHl2luu5656sBAUFSbFixTwOAABgJ8fCTrt27eTgwYOyb98+99G4cWNTrOz6OjAwUGJjY93fc/z4cTPVvHnz5ua5PurP0NDksm7dOhNeQkNDHXlfAADAuzhWs1O0aFGpU6eOx7nChQubNXVc5wcMGCAjR46UEiVKmAAzdOhQE3CaNWtmrnfo0MGEmn79+kl0dLSp0xk/frwpetbeGwAAgP+oZ6d69epy4cKFW87rkJNeyy6zZs2SLl26mMUEW7dubYamPvroI/f1fPnyyapVq8yjhqC+ffvKk08+KZMmTcq2NgAAAN/ml5GRkfFbv8nf39/0ouiKxTfXy+jUcJ3a7Ut06nlwcLApVqZ+B4Atllbwc7oJyEVPxP/mj3Ofd6ef379pGOuTTz5xf/3555+bF8i8mrHW11StWvU/bTMAAEC2+01hp3v37uZR97Dq37+/xzUtJtagM2PGjOxtIQAAQG6FnfT0dPNYrVo1sxBgqVKl7ua1AQAAvHM21qlTp7K/JQAAAN409Vzrc/TQNW5cPT4uCxYsyI62AQAAOBN2dF8pnd6tC/+VL1/e1PAAAABYE3ZiYmJk0aJFZjE/AAAA6xYV1D2rWrRokf2tAQAA8IaenT/96U+ydOlSeemll7K7Pchl+aJbOd0E5KIbY79wugkA4Bth5+rVq/Lmm2/K+vXrpV69emaNncxmzpyZXe0DAADI/bBz4MABadCggfn60KFDHtcoVgYAAD4fdjZu3Jj9LQEAAPCWAmUAAACre3batGnzi8NVGzZsuJs2AQAAOBt2XPU6LteuXZN9+/aZ+p2bNwgFAADwubAza9asLM+//PLLcvny5bttEwAAgHfW7PTt25d9sQAAgL1hJy4uTgoUKJCdPxIAACD3h7EiIiI8nmdkZMjZs2dlz549rKoMAAB8P+wEBwd7PPf395f77rvP7ITeoUOH7GobAACAM2Fn4cKFd//KAAAA3hp2XPbu3StHjx41X99///3ywAMPZFe7AAAAnAs7iYmJ0rt3b9m0aZOEhISYc0lJSWaxwWXLlknp0qWzp3UAAABOzMYaOnSoXLp0SQ4fPiwXL140hy4omJKSIs8999zdtgkAAMDZnp01a9bI+vXrpXbt2u5zoaGh8vrrr1OgDAAAfL9nJz09XQIDA285r+f0GgAAgE+HnbZt28qwYcMkPj7efe6HH36QESNGSLt27bKzfQAAALkfdl577TVTn1O1alW55557zFGtWjVzbu7cuXfXIgAAAKdrdipVqiRffvmlqds5duyYOaf1O2FhYdnZNgAAgNzt2dmwYYMpRNYeHD8/P2nfvr2ZmaVHkyZNzFo7W7duvftWAQAAOBF2Zs+eLQMHDpRixYpluYXEn//8Z5k5c2Z2tQ0AACB3w87+/fvloYceuu11nXauqyoDAAD4ZNhJSEjIcsq5S0BAgJw/fz472gUAAJD7Yed3v/udWSn5dg4cOCDly5fPjnYBAADkftjp1KmTvPTSS3L16tVbrv38888yceJE6dKlS/a0DAAAILenno8fP14++ugjqVmzpgwZMkTuu+8+c16nn+tWETdu3JAXX3wxO9oFAACQ+2GnbNmysn37dhk0aJBERkZKRkaGOa/T0MPDw03g0XsAAAC8xW9eVLBKlSryz3/+U3788Uc5efKkCTw1atSQ4sWL50wLAQAAcnsFZaXhRhcSBAAAsG5vLAAAAF9B2AEAAFZzNOy88cYbUq9ePbP9hB7NmzeX1atXu6/rFPfBgwdLyZIlpUiRItKjRw+zsGFmp0+fls6dO0uhQoWkTJkyMmbMGLl+/boD7wYAAHgjR8NOxYoVZdq0aWaLiT179kjbtm2lW7ducvjwYXN9xIgR8umnn8ry5ctl8+bNEh8fLxEREe7v16nuGnTS0tLMLLHFixfLokWLZMKECQ6+KwAA4E38Mlzzx71EiRIl5K9//av07NlTSpcuLUuXLjVfu9bzqV27tsTFxUmzZs1ML5AuYqghyDXlPSYmRsaNG2e2rcifP/8dvabu4q4bmSYnJ2e5yanN8kW3croJyEU3xn7hdBOQi5ZW8HO6CchFT8R71cd5rrjTz2+vqdnRXpply5bJlStXzHCW9vZcu3ZNwsLC3PfUqlVLKleubMKO0se6det6rO2j6/3om3f1DmUlNTXV3JP5AAAAdnI87Bw8eNDU4wQFBclf/vIXWblypYSGhsq5c+dMz0xISIjH/Rps9JrSx5sXMXQ9d92TlaioKJMEXUelSpVy5L0BAADnOR52dMuJffv2yc6dO83KzP3795cjR47k6Gvq6s/a5eU6zpw5k6OvBwAAfHBRweyivTf33nuv+bpRo0aye/du+dvf/ia9evUyhcdJSUkevTs6G6tcuXLma33ctWuXx89zzdZy3ZMV7UXSAwAA2M/xnp2bpaenm5oaDT6BgYESGxvrvnb8+HEz1VxrepQ+6jBYYmKi+55169aZIiUdCgMAAHC0Z0eHkzp27GiKji9dumRmXm3atEk+//xzU0szYMAAGTlypJmhpQFm6NChJuDoTCzVoUMHE2r69esn0dHRpk5Hd2bXtXnouQEAAI6HHe2RefLJJ+Xs2bMm3OgCgxp02rdvb67PmjVL/P39zWKC2tujM63mzZvn/v58+fLJqlWrTK2PhqDChQubmp9JkyY5+K4AAIA38bp1dpzAOjvIK1hnJ29hnZ28hXV2ivlOzQ4AAEB2IuwAAACrEXYAAIDVCDsAAMBqhB0AAGA1wg4AALAaYQcAAFiNsAMAAKxG2AEAAFYj7AAAAKsRdgAAgNUIOwAAwGqEHQAAYDXCDgAAsBphBwAAWI2wAwAArEbYAQAAViPsAAAAqxF2AACA1Qg7AADAaoQdAABgNcIOAACwGmEHAABYjbADAACsRtgBAABWI+wAAACrEXYAAIDVCDsAAMBqhB0AAGA1wg4AALAaYQcAAFiNsAMAAKxG2AEAAFYj7AAAAKsRdgAAgNUIOwAAwGqEHQAAYDXCDgAAsBphBwAAWI2wAwAArEbYAQAAVnM07ERFRUmTJk2kaNGiUqZMGenevbscP37c456rV6/K4MGDpWTJklKkSBHp0aOHJCQkeNxz+vRp6dy5sxQqVMj8nDFjxsj169dz+d0AAABv5GjY2bx5swkyO3bskHXr1sm1a9ekQ4cOcuXKFfc9I0aMkE8//VSWL19u7o+Pj5eIiAj39Rs3bpigk5aWJtu3b5fFixfLokWLZMKECQ69KwAA4E38MjIyMsRLnD9/3vTMaKhp3bq1JCcnS+nSpWXp0qXSs2dPc8+xY8ekdu3aEhcXJ82aNZPVq1dLly5dTAgqW7asuScmJkbGjRtnfl7+/Pl/9XVTUlIkODjYvF6xYsUkL8kX3crpJiAX3Rj7hdNNQC5aWsHP6SYgFz0R7zUf57nmTj+/vapmRxurSpQoYR737t1renvCwsLc99SqVUsqV65swo7Sx7p167qDjgoPDze/gMOHD2f5OqmpqeZ65gMAANjJa8JOenq6DB8+XFq2bCl16tQx586dO2d6ZkJCQjzu1WCj11z3ZA46ruuua7erFdIk6DoqVaqUQ+8KAAA4zWvCjtbuHDp0SJYtW5bjrxUZGWl6kVzHmTNncvw1AQCAMwLECwwZMkRWrVolW7ZskYoVK7rPlytXzhQeJyUlefTu6Gwsvea6Z9euXR4/zzVby3XPzYKCgswBAADs52jPjtZGa9BZuXKlbNiwQapVq+ZxvVGjRhIYGCixsbHuczo1XaeaN2/e3DzXx4MHD0piYqL7Hp3ZpYVKoaGhufhuAACANwpweuhKZ1r9z//8j1lrx1Vjo3U0BQsWNI8DBgyQkSNHmqJlDTBDhw41AUdnYimdqq6hpl+/fhIdHW1+xvjx483PpvcGAAA4GnbeeOMN8/hf//VfHucXLlwof/zjH83Xs2bNEn9/f7OYoM6i0plW8+bNc9+bL18+MwQ2aNAgE4IKFy4s/fv3l0mTJuXyuwEAAN7Iq9bZcQrr7CCvYJ2dvIV1dvIW1tkp5v2zsQAAAHICYQcAAFiNsAMAAKxG2AEAAFYj7AAAAKsRdgAAgNUIOwAAwGqEHQAAYDXCDgAAsBphBwAAWI2wAwAArEbYAQAAViPsAAAAqxF2AACA1Qg7AADAaoQdAABgNcIOAACwGmEHAABYjbADAACsRtgBAABWI+wAAACrEXYAAIDVCDsAAMBqhB0AAGA1wg4AALAaYQcAAFiNsAMAAKxG2AEAAFYj7AAAAKsRdgAAgNUIOwAAwGqEHQAAYDXCDgAAsBphBwAAWI2wAwAArEbYAQAAViPsAAAAqxF2AACA1Qg7AADAaoQdAABgNcIOAACwmqNhZ8uWLdK1a1epUKGC+Pn5yccff+xxPSMjQyZMmCDly5eXggULSlhYmJw4ccLjnosXL0qfPn2kWLFiEhISIgMGDJDLly/n8jsBAADeKsDJF79y5YrUr19fnn76aYmIiLjlenR0tMyZM0cWL14s1apVk5deeknCw8PlyJEjUqBAAXOPBp2zZ8/KunXr5Nq1a/LUU0/JM888I0uXLnXgHfmed2dvc7oJyE1jnW4AAOSxsNOxY0dzZEV7dWbPni3jx4+Xbt26mXPvvPOOlC1b1vQA9e7dW44ePSpr1qyR3bt3S+PGjc09c+fOlU6dOsmrr75qeowAAEDe5rU1O6dOnZJz586ZoSuX4OBgadq0qcTFxZnn+qhDV66go/R+f39/2blz521/dmpqqqSkpHgcAADATl4bdjToKO3JyUyfu67pY5kyZTyuBwQESIkSJdz3ZCUqKsoEJ9dRqVKlHHkPAADAeV4bdnJSZGSkJCcnu48zZ8443SQAAJDXwk65cuXMY0JCgsd5fe66po+JiYke169fv25maLnuyUpQUJCZvZX5AAAAdvLasKOzrzSwxMbGus9pbY3W4jRv3tw818ekpCTZu3ev+54NGzZIenq6qe0BAABwdDaWrodz8uRJj6Lkffv2mZqbypUry/Dhw2XKlClSo0YN99RznWHVvXt3c3/t2rXloYcekoEDB0pMTIyZej5kyBAzU4uZWAAAwPGws2fPHmnTpo37+ciRI81j//79ZdGiRTJ27FizFo+um6M9OK1atTJTzV1r7KglS5aYgNOuXTszC6tHjx5mbR4AAADll6EL2uRxOjyms7K0WDmv1e8sreDndBOQi56Iz/N/7nkKf995S178+065w89vr63ZAQAAyA6EHQAAYDXCDgAAsBphBwAAWI2wAwAArEbYAQAAViPsAAAAqxF2AACA1Qg7AADAaoQdAABgNcIOAACwGmEHAABYjbADAACsRtgBAABWI+wAAACrEXYAAIDVCDsAAMBqhB0AAGA1wg4AALAaYQcAAFiNsAMAAKxG2AEAAFYj7AAAAKsRdgAAgNUIOwAAwGqEHQAAYDXCDgAAsBphBwAAWI2wAwAArEbYAQAAViPsAAAAqxF2AACA1Qg7AADAaoQdAABgNcIOAACwGmEHAABYjbADAACsRtgBAABWI+wAAACrEXYAAIDVCDsAAMBq1oSd119/XapWrSoFChSQpk2byq5du5xuEgAA8AJWhJ33339fRo4cKRMnTpQvv/xS6tevL+Hh4ZKYmOh00wAAgMOsCDszZ86UgQMHylNPPSWhoaESExMjhQoVkgULFjjdNAAA4LAA8XFpaWmyd+9eiYyMdJ/z9/eXsLAwiYuLy/J7UlNTzeGSnJxsHlNSUiSv+Snd6RYgN+XFf+N5GX/feUte/PtO+f/vOSMjw+6w869//Utu3LghZcuW9Tivz48dO5bl90RFRckrr7xyy/lKlSrlWDsBbzAwONjpJgDIIXn57/vSpUsS/Avv3+fDzn9Ce4G0xsclPT1dLl68KCVLlhQ/Pz9H24bc+S8BDbZnzpyRYsWKOd0cANmIv++8JSMjwwSdChUq/OJ9Ph92SpUqJfny5ZOEhASP8/q8XLlyWX5PUFCQOTILCQnJ0XbC++j/EfJ/hoCd+PvOO4LvoEfL5wuU8+fPL40aNZLY2FiPnhp93rx5c0fbBgAAnOfzPTtKh6T69+8vjRs3lgcffFBmz54tV65cMbOzAABA3mZF2OnVq5ecP39eJkyYIOfOnZMGDRrImjVrbilaBpQOYeqaTDcPZQLwffx9Iyt+Gb82XwsAAMCH+XzNDgAAwC8h7AAAAKsRdgAAgNUIOwAAwGqEHQAAYDXCDvKMkydPyueffy4///yzec5ERADIGwg7sN6FCxckLCxMatasKZ06dZKzZ8+a8wMGDJBRo0Y53TwAQA4j7MB6I0aMkICAADl9+rQUKlTIYzFKXXwSgO/bunWr9O3b12wT9MMPP5hz7777rnzxxRdONw1egLAD661du1amT58uFStW9Dhfo0YN+e677xxrF4DssWLFCgkPD5eCBQvK//7v/0pqaqo5n5ycLFOnTnW6efAChB1YT/dJy9yj43Lx4kWWlAcsMGXKFImJiZH58+dLYGCg+3zLli3lyy+/dLRt8A6EHVjv97//vbzzzjvu535+fpKeni7R0dHSpk0bR9sG4O4dP35cWrdufcv54OBgSUpKcqRN8C5WbAQK/BINNe3atZM9e/ZIWlqajB07Vg4fPmx6drZt2+Z08wDcpXLlypnZllWrVvU4r/U61atXd6xd8B707MB6derUka+++kpatWol3bp1M8NaERERZmz/nnvucbp5AO7SwIEDZdiwYbJz507TcxsfHy9LliyR0aNHy6BBg5xuHrwAu54DAHyafoxpIXJUVJT89NNP5pzW42nYmTx5stPNgxcg7MBKBw4cuON769Wrl6NtAZA7dJhah7MuX74soaGhUqRIEaebBC9B2IGV/P39TXf2r/3z1ntu3LiRa+0CAOQ+CpRhpVOnTjndBAA5SOvu7tRHH32Uo22B9yPswEpVqlRxugkAcpBOKwfuFMNYyDOOHDlitozQcf3MHn74YcfaBADIefTswHrffPONPPLII3Lw4EGPOh79WlGzAwB2I+zAerr+RrVq1SQ2NtY87tq1y+yErjuev/rqq043D0A2+PDDD+WDDz7IsveWLSPAooKwXlxcnEyaNElKlSplZmnpoQsM6poczz33nNPNA3CX5syZI0899ZSULVvWLBb64IMPSsmSJU2vbseOHZ1uHrwAYQfW02GqokWLmq818Ojqqq4iZt1TB4Bvmzdvnrz55psyd+5cyZ8/v9kSZt26deY/ZnTnc4CwgzyxXcT+/fvN102bNjV7ZemeWNrbw745gO/ToasWLVqYrwsWLCiXLl0yX/fr10/+8Y9/ONw6eAPCDqw3fvx4s8u50oCja/DoTuj//Oc/Tfc3AN/fCFQ39lWVK1eWHTt2mK/1b50Jx1AUKMN64eHh7q/vvfdeOXbsmPk/xuLFi7tnZAHwXW3btpVPPvlEHnjgAVO7M2LECFOwvGfPnt+0+CDsxTo7AACfpj23egQE/Pu/399//30zVF2jRg35y1/+IoGBgU43EQ4j7MB6V69eNYWLGzdulMTERPeQlgvTUgE7/s51A+Cb/8a197Zr166Otg3OYxgL1hswYICsXbtWevbsaaakMnQF2GXNmjWmGFnXz7oZm/1C0bODPLGHjhYjt2zZ0ummAMgBOlzVoUMHmTBhgllrB7gZs7Fgvd/97nfudXYA2CchIUFGjhxJ0MFtEXZgvRkzZsi4cePku+++c7opAHKADlFv2rTJ6WbAizGMBeudP39eHnvsMdmyZYsUKlTolpkZrvU5APimn376SR599FEpXbq01K1b95a/cbaFAWEH1gsLCzMrrGqhsnZz31yg3L9/f8faBuDuvf3222aKeYECBcyeWJn/xvVr3SMLeRthB9bT3hzdDLR+/fpONwVADq2grL03zz//vNnoF7gZ/ypgvVq1asnPP//sdDMA5JC0tDTp1asXQQe3xb8MWG/atGkyatQoU8Co63CkpKR4HAB8mw5F66rJwO0wjAXruf5r7+ZaHf2nz4JjgO/TIax33nnHDFXXq1fvlgLlmTNnOtY2eAdWUIb1dJsIAPY6ePCg2QRUHTp0yOMaK6ZD0bMDAACsRs0O8oStW7dK3759pUWLFvLDDz+Yc++++6588cUXTjcNAJDDCDuw3ooVKyQ8PFwKFixodjhPTU0155OTk2Xq1KlONw8AkMMIO7DelClTJCYmRubPn+9RuKgbg2r4AQDYjbAD6x0/flxat26d5W7oSUlJjrQJAJB7CDvIE6urnjx58pbzWq9TvXp1R9oEAMg9hB1Yb+DAgTJs2DDZuXOnmYYaHx8vS5YskdGjR8ugQYOcbh4AIIexzg6sdODAAalTp45ZUDAyMlLS09OlXbt2ZndkHdIKCgoyYWfo0KFONxUAkMNYZwdWypcvn5w9e1bKlCljhqp2794tRYsWNcNZly9fltDQUClSpIjTzQQA5AJ6dmClkJAQOXXqlAk73377renZyZ8/vwk5AIC8hbADK/Xo0UP+8Ic/SPny5U2dTuPGjU1vT1a++eabXG8fACD3EHZgpTfffFMiIiLMsJVuEqhFyjqMBQDIe6jZgfWeeuopmTNnDmEHAPIowg4AALAa6+wAAACrEXYAAIDVCDsAAMBqhB0AAGA1wg4AALAaYQeATzp//rzZyLVy5cpmrzPd3T48PFy2bdvmdNMAeBkWFQTgs6tkp6WlyeLFi83+ZwkJCRIbGysXLlxwumkAvAw9OwB8TlJSkmzdulWmT58ubdq0kSpVqsiDDz5odrh/+OGH3ff86U9/ktKlS0uxYsWkbdu2sn//fnevkPYETZ061f0zt2/fbvZP08AEwC6EHQA+R3es1+Pjjz+W1NTULO959NFHJTExUVavXi179+6Vhg0bSrt27eTixYsmAC1YsEBefvll2bNnj1y6dEn69esnQ4YMMfcAsAsrKAPwSStWrDB7nv38888myOjGr71795Z69erJF198IZ07dzZhR+t5XO69914ZO3asPPPMM+b54MGDZf369Waj2IMHD8ru3bs97gdgB8IOAJ919epVM5y1Y8cO04Oza9cueeutt+TKlStmA9iCBQt63K/BaPTo0Wb4y/W8Tp06cubMGdP7U7duXYfeCYCcRNgBYA2t0Vm3bp08++yzMnfuXNm0adMt94SEhEipUqXM14cOHZImTZrItWvXZOXKldK1a1cHWg0gpzEbC4A1QkNDTR2PDmudO3dOAgICpGrVqlneqzO5+vbtK7169ZL77rvPBCUdyipTpkyutxtAzqJnB4DP0enlWoD89NNPmxqdokWLmkLjoUOHmlodHcpq3bq1KTyOjo6WmjVrSnx8vHz22WfyyCOPmBqdMWPGyIcffmhmaGmxs9b8BAcHy6pVq5x+ewCyGWEHgM/RGVg6k2rt2rXy9ddfm2GoSpUqmQD0wgsvmFodDTovvviiKWR2TTXXABQVFWW+p3379rJx40Zp1aqV+Znffvut1K9fX6ZNm2YWKwRgD8IOAACwGuvsAAAAqxF2AACA1Qg7AADAaoQdAABgNcIOAACwGmEHAABYjbADAACsRtgBAABWI+wAAACrEXYAAIDVCDsAAEBs9v8ANPV1zia4itMAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_df.groupby(['sex', 'survived']).size().unstack().plot(\n",
    "    kind='bar', stacked=True, color=['#a51900', '#02893b'], xlabel=\"Sex\", ylabel=\"Count\"\n",
    ")\n",
    "plt.legend(['No', 'Yes'], title=\"Survived\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2dc6b6-a871-4509-8049-52d84833c417",
   "metadata": {},
   "source": [
    "**Task 1(a)**: <br />\n",
    "**(i)** Considering the above visualisations, are there any trends or patterns that you can identify in the data? <br />\n",
    "**(ii)** Without having access to any particular model or the associated explanations, which features would you expect to be the most and least important for a neural network trained on the dataset? How can you tell and how certain can you be of your assessment? <br />\n",
    "**(iii)** Apart from inspecting the above plots, is there anything else you could do as part of the exploratory analysis that would allow you to better understand the data and the behaviour of the models trained on it? <br />\n",
    "Please write your answers in a few sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a7e4a2-988d-4580-9fb9-a93f76c3164b",
   "metadata": {},
   "source": [
    "## Model Initialisation and Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4435873c-e279-45c9-b2b0-38faedba32ac",
   "metadata": {},
   "source": [
    "First, we define a global device variable to enable running this code on a GPU or a CPU, as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "65a56bef-3daa-44aa-8e57-d001b3bff2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a96541-7047-4c2b-845f-b823f4a8b239",
   "metadata": {},
   "source": [
    "Here, we define several utility functions for constructing, training and evaluating neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "395dd993-e2e1-46a8-b727-9f646ce4e409",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torcheval.metrics.functional import binary_f1_score, binary_accuracy, binary_auroc\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def construct_nn(nn_dims, activation_fun):\n",
    "    \"\"\"\n",
    "    Constructs a neural network with the specified architecture.\n",
    "    \"\"\"\n",
    "    layers = []\n",
    "    for i in range(1, len(nn_dims)):\n",
    "        in_dim, out_dim = nn_dims[i-1], nn_dims[i]\n",
    "        layers.append(nn.Linear(in_dim, out_dim))\n",
    "        layers.append(activation_fun())\n",
    "    # Remove the last activation layer and add Sigmoid instead\n",
    "    layers = layers[:-1]\n",
    "    layers.append(nn.Sigmoid())\n",
    "    \n",
    "    return nn.Sequential(*layers).to(DEVICE)\n",
    "\n",
    "def train_nn(model, train_dl, num_epochs=100):\n",
    "    \"\"\"\n",
    "    Trains a neural network using the data from the provided data loader.\n",
    "    \"\"\"\n",
    "    loss_fun = nn.BCELoss()\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=0.005)\n",
    "    model.train()\n",
    "    \n",
    "    losses = []\n",
    "    for epoch in tqdm(range(num_epochs), leave=False):\n",
    "        total_loss = 0\n",
    "        for i, (x, y) in list(enumerate(train_dl)):\n",
    "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "            opt.zero_grad()\n",
    "            out = model(x)\n",
    "            loss = loss_fun(out.squeeze(-1), y.float())\n",
    "            total_loss += loss.item()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "        losses.append(total_loss)\n",
    "\n",
    "def eval_nn(model, test_dataset):\n",
    "    \"\"\"\n",
    "    Evaluates binary classification performance of a model on the given\n",
    "    test dataset.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    loss_fun = nn.BCELoss()\n",
    "    predictions = model(test_dataset.samples.to(DEVICE))\n",
    "    labels = test_dataset.labels.unsqueeze(-1).to(DEVICE)\n",
    "    loss = loss_fun(predictions, labels.float()).item()\n",
    "\n",
    "    predictions = predictions.squeeze(-1).detach()\n",
    "    labels = labels.squeeze(-1).detach()\n",
    "    f1 = binary_f1_score(predictions, labels).item()\n",
    "    accuracy = binary_accuracy(predictions, labels).item()\n",
    "    auc = binary_auroc(predictions, labels).item()\n",
    "\n",
    "    return loss, f1, accuracy, auc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3495eeb-14fa-4ae2-aaaf-b24c64ab8ef9",
   "metadata": {},
   "source": [
    "In this cell, we initialise and train the neural model that we will be explaining in this coursework. For a real-world application, you would typically wish to perform a full hyperparameter search in order to identify the most effective model architecture. However, achieving a maximum performance is not the objective of this coursework, so we just pre-define a model that performs reasonably well on the given task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "68254f96-7821-4a53-8615-2edbfee294ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â€”â€”â€”â€”â€”â€”â€”[ Model training ]â€”â€”â€”â€”â€”â€”â€”\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70bb4639b6224ff9b6c56762dcb2280a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed!\n",
      "\n",
      "â€”â€”â€”â€”â€”â€”â€”[ Evaluation ]â€”â€”â€”â€”â€”â€”â€”\n",
      "F1 score: 0.75\n",
      "Accuracy: 0.80\n",
      "AUC: 0.82\n"
     ]
    }
   ],
   "source": [
    "def print_metric(name, value):\n",
    "    print(f\"{name}: {'{:.2f}'.format(round(value, 2))}\")\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "model = construct_nn([7, 256, 256, 1], nn.ReLU).to(DEVICE)\n",
    "\n",
    "print(\"â€”â€”â€”â€”â€”â€”â€”[ Model training ]â€”â€”â€”â€”â€”â€”â€”\")\n",
    "train_nn(model, train_dl, num_epochs=1000)\n",
    "print(\"Training completed!\")\n",
    "print()\n",
    "\n",
    "print(\"â€”â€”â€”â€”â€”â€”â€”[ Evaluation ]â€”â€”â€”â€”â€”â€”â€”\")\n",
    "test_loss, f1, accuracy, auc = eval_nn(model, test_dataset)\n",
    "print_metric(\"F1 score\", f1)\n",
    "print_metric(\"Accuracy\", accuracy)\n",
    "print_metric(\"AUC\", auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6034c1b-500d-4e02-a6e7-742d62208c8e",
   "metadata": {},
   "source": [
    "## Feature Attributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fa2e83-1342-4a8c-b1e7-d1188455eeb6",
   "metadata": {},
   "source": [
    "In this section of the coursework, you will implement SHAP as introduced in the lectures and conduct additional experiments with various feature attribution methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579e7328-82bd-41aa-bcb9-6488e575c3f4",
   "metadata": {},
   "source": [
    "### SHAP Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecf73aa-f5e3-4c36-9875-19e38c5d5c99",
   "metadata": {},
   "source": [
    "**Task 2(a)(i)**: As a first step in implementing SHAP, define a `compute_coefficient` function to compute the SHAP coalition coefficient/weight as specified by the formula from the lectures:\n",
    "\n",
    "$$g_{SHAP}(\\mathcal{M},\\mathbf{x},i) = \\sum_{\\mathbf{z} \\subseteq \\mathbf{x}}{\\frac{|\\mathbf{z}|!(n - |\\mathbf{z}| - 1)! }{n!} \\mathcal{M}( \\mathbf{z}) - \\mathcal{M}(\\mathbf{z}_{-i})} \\nonumber$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564deb60-0275-4b75-a2a8-b2e61ef4563c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def compute_coefficient(num_in_coalition, total_features):\n",
    "    \"\"\"\n",
    "    Computes the SHAP coefficient for a coalition.\n",
    "\n",
    "    Parameters:\n",
    "        num_in_coalition (int): The number of features in the given coalition\n",
    "        total_features (int): The total number of considered features\n",
    "\n",
    "    Returns:\n",
    "        coefficient (float): The SHAP weight for the given coalition\n",
    "    \"\"\"\n",
    "    return (math.factorial(num_in_coalition) * math.factorial(total_features-num_in_coalition-1))/(math.factorial(total_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8131b77-5eac-471c-a1a8-ff138f4b4541",
   "metadata": {},
   "source": [
    "**Task 2(a)(ii)**: Next, define a function `generate_coalitions`, which will return the list representing all the possible coalitions for a possible feature.\n",
    "\n",
    "Hint #1: You may find it helpful to use [itertools](https://docs.python.org/3/library/itertools.html) and [Python generators](https://wiki.python.org/moin/Generators) for implementing this function.\n",
    "\n",
    "Hint #2: Passing a full list of feature IDs is not strictly necessary here, but you will find this list helpful for implementing other functions, so we also recommend taking it as a parameter here. As an example, for the Titanic dataset, this list could look like `[0, 1, 2, 3, 4, 5, 5]` (note the repeated `5` for the one-hot-encoded `sex` feature)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e60209-d01b-4947-bba5-e13b5385d43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def generate_coalitions(feature_ids, target_feature_id):\n",
    "    \"\"\"\n",
    "    Generates the possible feature coalitions for the purpose of computing the Shapley value\n",
    "    for the target feature.\n",
    "\n",
    "    Parameters:\n",
    "        feature_ids (list): A list with feature IDs from 0 to N (where N is\n",
    "            the total number of features) identifying the used features. Distinct\n",
    "            columns for one-hot-encoded features should be assigned the same\n",
    "            numerical ID.\n",
    "        target_feature_id (int): The ID of the removed feature for which the coalitions\n",
    "            should be generated.\n",
    "\n",
    "    Retruns:\n",
    "        coalitions (list): A nested list structure of coalitions in the form:\n",
    "            [(set(coalition 1 in features set), set(coalition 1 out features set)), ...].\n",
    "            Note that feature_id should not appear in either of the in/out lists.\n",
    "    \"\"\"\n",
    "    remaining_features = [f for f in feature_ids if f != target_feature_id]\n",
    "    coalitions = []\n",
    "    for size in range(len(remaining_features)+1):\n",
    "        for subset in itertools.combinations(remaining_features, size):\n",
    "            included = set(subset)\n",
    "            excluded = set(remaining_features) - included\n",
    "            coalitions.append((included, excluded))\n",
    "    return coalitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6fe694-5010-47c6-a3dc-68ffd032eb1b",
   "metadata": {},
   "source": [
    "**Task 2(a)(iii)**: Next, implement a function `delete_features` that deletes the specified features from the given input tensor `x`. In contrast with the setting in the SHAP tutorial, the majority of features considered in this coursework are non-binary, which makes the deletion of features slightly more challenging. The general procedure for performing the deletion can be described as follows:\n",
    "1. For each sample in `x` and each deleted feature, randomly sample the value of the deleted feature from another data point in the background dataset\n",
    "2. If the sampled value is identical to the current value of the deleted feature, continue sampling new values until finding one that differs. This ensures that feature deletion actually changes the values of categorical variables or variables with few possible values.\n",
    "3. Replace the value of the deleted feature in the currently considered sample with the newly sampled value\n",
    "\n",
    "Hint #1: Boolean tensor masks \"selecting\" certain features can be very helpful here.\n",
    "\n",
    "Hint #2: Make sure not to overwrite values in the original `x` when deleting features. Instead, the function should return a new tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75928ee-3f2b-4427-8f41-45cfbe239eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_features(x, background_dataset, feature_ids, deleted_feature_ids):\n",
    "    \"\"\"\n",
    "    Deletes the specified features from inputs x using the background dataset.\n",
    "\n",
    "    Parameters:\n",
    "        x (Tensor): A tensor of inputs with the shape (batch_size, num_features).\n",
    "        background_dataset (Tensor): A tensor of background data samples with the same shape as x.\n",
    "        feature_ids (list): A list with feature IDs, same as in generate_coalitions.\n",
    "        deleted_feature_ids (set): A set with feature IDs to be deleted from x.\n",
    "\n",
    "    Returns:\n",
    "        x_deleted (tensor): A new tensor of inputs with the specified features deleted.\n",
    "    \"\"\"\n",
    "    # Make the sampling deterministic\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # First create x_deleted as a copy of x\n",
    "    x_deleted = x.clone()\n",
    "\n",
    "    for deleted_id in deleted_feature_ids:\n",
    "\n",
    "        # Loop through each row in x_deleted, delete (replace) each feature specified in deleted_feature_ids\n",
    "        for i in range(x.shape[0]):\n",
    "            original_value = x[i, deleted_id]\n",
    "\n",
    "            # Randomly sample this value from background_dataset until it's different\n",
    "            sampled_value = original_value\n",
    "            while sampled_value == original_value:\n",
    "                background_column = background_dataset[:,deleted_id]\n",
    "                sampled_value = background_column[torch.randint(0, len(background_column), (1,))].item()\n",
    "                \n",
    "            x_deleted[i, deleted_id] = sampled_value\n",
    "\n",
    "    return x_deleted\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679f0dc8-1542-4989-b148-41d5e5889e5b",
   "metadata": {},
   "source": [
    "**Task 2(a)(iv)**: Finally, put everything together in the `shap_attribute` function, which will compute the SHAP attributions for the given input and model. Note that the function also takes in a `target_idx` specifying for which output neuron the explanations should be computed. This is not strictly necessary for the Titanic model, which only has a single Sigmoid output, but will be needed once you start working with a more complex model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecaa2ba-f48f-474e-9503-d22598e554e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shap_attribute(model, x, background_dataset, feature_ids, target_idx=0):\n",
    "    \"\"\"\n",
    "    Computes the SHAP attributions for the given input and model.\n",
    "\n",
    "    Parameters:\n",
    "        model (Object): A PyTorch model for which the attributions should be computed.\n",
    "        x (Tensor): Inputs for which the explanations should be computed, in shape (batch_size, num_features).\n",
    "        background_dataset (Tensor): A tensor of background data samples with the same shape as x.\n",
    "        feature_ids (list): A list with feature IDs, same as in generate_coalitions.\n",
    "        target_idx (int): The ID of the target neuron for which to compute an explanation\n",
    "            (useful for classification tasks with multiple labels where it should correspond\n",
    "\n",
    "    Returns:\n",
    "        attributions (Tensor): A tensor of SHAP attributions, with the same shape as the input\n",
    "    \"\"\"\n",
    "    # Initialise attributions as a tentor with same shape as x\n",
    "    attributions = torch.empty(x.shape, dtype=torch.float32)\n",
    "    \n",
    "    # Convert target_idx to tensor if it's not already\n",
    "    if isinstance(target_idx, int):\n",
    "        target_idx = torch.full((x.shape[0],), target_idx)\n",
    "\n",
    "    # Loop through each row\n",
    "    for i in range(x.shape[0]):\n",
    "        \n",
    "        current_target = target_idx[i] if torch.is_tensor(target_idx) else target_idx\n",
    "\n",
    "        # Loop through each feature in the current sample (i.e. row of x), calculate shapley score\n",
    "        sample_x = x[i].unsqueeze(0)\n",
    "        for j in range(x.shape[1]):\n",
    "            \n",
    "            g_shap = 0\n",
    "            coalitions = generate_coalitions(feature_ids, j)\n",
    "            for coalition in coalitions:\n",
    "                included_features, excluded_features = coalition\n",
    "\n",
    "                # Construct z and z-i\n",
    "                sample_with_coalition = delete_features(sample_x, background_dataset, feature_ids, excluded_features)\n",
    "                sample_deleted_feature = delete_features(sample_with_coalition, background_dataset, feature_ids, {j})\n",
    "                \n",
    "                # Calculate M(z) and M(z-i)\n",
    "                pred_with = model(sample_with_coalition)[:, current_target]\n",
    "                pred_without = model(sample_deleted_feature)[:, current_target]\n",
    "                \n",
    "                # Calculate g_shap\n",
    "                shap_weight = compute_coefficient(len(included_features), len(feature_ids))\n",
    "                g_shap += shap_weight * (pred_with - pred_without)\n",
    "\n",
    "            attributions[i,j] = g_shap\n",
    "\n",
    "    return attributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3017834b-bf1c-48e4-ab1c-ff0d26ccab5c",
   "metadata": {},
   "source": [
    "### Additional Explanation Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33325644-8c95-4e9f-aa94-9cf9206d2268",
   "metadata": {},
   "source": [
    "Apart from SHAP, which you just implemented, you will also be experimenting with two more feature attribution methods implemented in the [Captum](https://captum.ai/) library â€” [Shapley Value Sampling](https://captum.ai/api/shapley_value_sampling.html) and [DeepLIFT](https://captum.ai/api/deep_lift.html). Shapley Value Sampling is a more computationally tractable approximation of SHAP and computes the scores by randomly sampling a fixed number of coalitions instead of considering all of them. Meanwhile, DeepLIFT is a fast gradient-based attribution method specifically designed for neural models. If you are interested, you can learn more about DeepLIFT in [its original paper](https://arxiv.org/abs/1704.02685).\n",
    "\n",
    "To get you started, we provide an example of how to use the Captum library to generate Shapley Value Sampling attributions for the first sample from the Titanic test set (note that the library also allows you to compute attributions for a batch of inputs):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974eca85-1658-4222-95fc-bf3d180baeb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.1617,  0.0000,  0.0000,  0.2395, -0.4373, -0.4373]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from captum.attr import ShapleyValueSampling\n",
    "\n",
    "# Note that this is similar to feature_ids from the implementation above,\n",
    "# but the shape and the data type are different\n",
    "feature_mask = torch.tensor([[0, 1, 2, 3, 4, 5, 5]]).to(DEVICE)\n",
    "svs = ShapleyValueSampling(model)\n",
    "attributions = svs.attribute(test_dataset.samples[[0]].to(DEVICE), target=0, feature_mask=feature_mask)\n",
    "attributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e108b2-15d8-4c0b-bf6f-a64a4fe3b9e2",
   "metadata": {},
   "source": [
    "Notice that, in contrast with the SHAP implementation above, we did not need to pass in the background dataset. This is because Captum takes a slightly different approach to deleting features and instead replaces them with a pre-specified baseline value (see the `baselines` parameter description in the [documentation](https://captum.ai/api/shapley_value_sampling.html)). For the purposes of this coursework, it is fine to use the default (zero) baseline for both Shapley Value Sampling and DeepLIFT."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c24e08-7871-44fe-afdd-d15c3c8d1fee",
   "metadata": {},
   "source": [
    "### Feature Attribution Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5d4669-00e6-43bb-826f-c0f12aa45f96",
   "metadata": {},
   "source": [
    "In this section, you will conduct several experiments associated with feature attribution methods.\n",
    "\n",
    "**Task 2(b)**: Using your implementation of SHAP and Captum implementations of Shapley Value Sampling and DeepLIFT, compute feature attributions for 10 randomly selected instances from the Titanic test set. Then answer the following questions: <br />\n",
    "**(i)** Which features generally seem to be the most important and least important for the explained model according to each of the explanations? <br />\n",
    "**(ii)** Are there any substantial differences between the different attribution methods? What might be the possible reasons for the different methods returning different attribution scores? <br />\n",
    "**(iii)** Do the attribution scores match your expectations for the most/least important features from task 1(a)(ii)? What might be the reasons for a user's expected explanations differing from the computed attribution explanations? <br />\n",
    "**(iv)** Considering the insights gained from the exploratory data analysis and the feature attribution explanations, as well as the definitions of the explanations themselves, what are the potential advantages/disadvantages of each of these methods when trying to understand the behaviour of a model on a particular dataset? <br />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8190035b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the format for showing result to keep clean\n",
    "torch.set_printoptions(sci_mode=False,linewidth=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2680c1c-9276-45c1-94ae-c10a660b74f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores by shap_attribute function from above:\n",
      "tensor([[    -0.0621,     -0.0525,      0.1464,     -0.0003,     -0.0097,     -0.0490,     -0.8823],\n",
      "        [     0.1720,      0.1027,      0.0016,     -0.2442,     -0.0380,      0.0300,      0.7109],\n",
      "        [    -0.0818,      0.0419,      0.1645,     -0.0246,     -0.0107,     -0.0667,     -1.1506],\n",
      "        [     0.0235,      0.0797,     -0.0716,      0.1694,     -0.1619,     -0.0217,     -0.2038],\n",
      "        [     0.1013,      0.0281,      0.1491,      0.0002,     -0.0055,     -0.0977,     -1.1359],\n",
      "        [    -0.0724,     -0.0529,      0.1502,     -0.0444,     -0.0126,     -0.0416,     -1.3554],\n",
      "        [     0.0930,      0.0198,     -0.0129,      0.0291,      0.0576,      0.1074,      1.5163],\n",
      "        [     0.1065,      0.0379,      0.1697,     -0.0753,      0.0053,     -0.0725,     -1.1343],\n",
      "        [     0.1810,      0.0699,      0.0422,     -0.0311,     -0.0727,     -0.1917,     -1.4688],\n",
      "        [     0.1803,     -0.0896,      0.0651,     -0.2203,     -0.0814,      0.0525,      1.2860]],\n",
      "       grad_fn=<CopySlices>)\n",
      "\n",
      "Sorted Feature Importance:\n",
      "Feature 6: 1.0844\n",
      "Feature 0: 0.1074\n",
      "Feature 2: 0.0973\n",
      "Feature 3: 0.0839\n",
      "Feature 5: 0.0731\n",
      "Feature 1: 0.0575\n",
      "Feature 4: 0.0455\n",
      "Scores by Shapley Value Sampling:\n",
      "tensor([[ 0.0000, -0.0012,  0.0000,  0.0000,  0.2472, -0.1375, -0.1375],\n",
      "        [ 0.5913, -0.0556,  0.2008,  0.0000,  0.1935,  0.0298,  0.0298],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.1936, -0.1913, -0.1913],\n",
      "        [ 0.0000,  0.0000, -0.0932, -0.0848,  0.2728, -0.1349, -0.1349],\n",
      "        [ 0.2440,  0.0000,  0.0000,  0.0000,  0.0097, -0.2417, -0.2417],\n",
      "        [ 0.0000,  0.2521,  0.0000,  0.0000,  0.1552, -0.4303, -0.4303],\n",
      "        [ 0.4849,  0.1989,  0.0000,  0.0000,  0.4255, -0.1494, -0.1494],\n",
      "        [ 0.2168, -0.0090,  0.0000,  0.0000,  0.0841, -0.3309, -0.3309],\n",
      "        [ 0.3481,  0.0000,  0.0000,  0.0000, -0.0086, -0.3796, -0.3796],\n",
      "        [ 0.4721,  0.1628,  0.1923,  0.0000,  0.2435, -0.1108, -0.1108]])\n",
      "\n",
      "Sorted Feature Importance:\n",
      "Feature 0: 0.2357\n",
      "Feature 5: 0.2136\n",
      "Feature 6: 0.2136\n",
      "Feature 4: 0.1834\n",
      "Feature 1: 0.0680\n",
      "Feature 2: 0.0486\n",
      "Feature 3: 0.0085\n",
      "Scores by DeepLIFT:\n",
      "tensor([[ 0.0000, -0.1832,  0.0000,  0.0000,  0.3777,  0.0000, -0.0860],\n",
      "        [ 0.6281,  0.0892,  0.1527,  0.0000, -0.0107,  0.1007, -0.0000],\n",
      "        [ 0.0000,  0.0000, -0.0000, -0.0000,  0.0783,  0.0000, -0.0759],\n",
      "        [-0.0000,  0.0000, -0.0223,  0.0063, -0.0206, -0.0000, -0.0035],\n",
      "        [ 0.0739,  0.0000,  0.0000,  0.0000,  0.0167,  0.0000, -0.0786],\n",
      "        [-0.0000, -0.0203, -0.0000,  0.0000,  0.0741, -0.0000, -0.0769],\n",
      "        [-0.2020, -0.0912, -0.0000, -0.0000,  1.0797,  0.1734, -0.0000],\n",
      "        [-0.0052, -0.0175,  0.0000,  0.0000,  0.0017,  0.0000, -0.0180],\n",
      "        [-0.0104, -0.0000, -0.0000,  0.0000, -0.0191,  0.0000, -0.0106],\n",
      "        [ 0.2021, -0.6310,  0.4949,  0.0000,  0.6090,  0.2848, -0.0000]], grad_fn=<MulBackward0>)\n",
      "\n",
      "Sorted Feature Importance:\n",
      "Feature 4: 0.2288\n",
      "Feature 0: 0.1122\n",
      "Feature 1: 0.1032\n",
      "Feature 2: 0.0670\n",
      "Feature 5: 0.0559\n",
      "Feature 6: 0.0349\n",
      "Feature 3: 0.0006\n",
      "Feature names after preprocessing: ['num__pclass' 'num__age' 'num__sibsp' 'num__parch' 'num__fare'\n",
      " 'cat__sex_female' 'cat__sex_male']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\MScAI\\ethics\\explainability-cw\\xaivenv\\Lib\\site-packages\\captum\\_utils\\gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n",
      "d:\\MScAI\\ethics\\explainability-cw\\xaivenv\\Lib\\site-packages\\captum\\attr\\_core\\deep_lift.py:304: UserWarning: Setting forward, backward hooks and attributes on non-linear\n",
      "               activations. The hooks and attributes will be removed\n",
      "            after the attribution is finished\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from captum.attr import ShapleyValueSampling, DeepLift\n",
    "\n",
    "# Randomly select 10 instances from Titanic test set\n",
    "test_samples = test_dataset.samples\n",
    "random_indices = torch.randperm(len(test_samples))[:10]\n",
    "random_sample_of_10 = test_samples[random_indices]\n",
    "\n",
    "\n",
    "# SHAP implementation from above\n",
    "background_indices = torch.randperm(len(test_dataset.samples))[:10]  \n",
    "background_dataset = test_dataset.samples[background_indices]  # Shape: (10, num_features)\n",
    "attributions_SHAP = shap_attribute(model, random_sample_of_10, background_dataset, feature_ids=[0, 1, 2, 3, 4, 5, 5], target_idx=0)\n",
    "print('Scores by shap_attribute function from above:')\n",
    "print(attributions_SHAP)\n",
    "\n",
    "feature_importance_SHAP = torch.mean(torch.abs(attributions_SHAP), dim=0)\n",
    "sorted_features_SHAP = sorted(enumerate(feature_importance_SHAP.tolist()), key=lambda x: x[1], reverse=True)\n",
    "print(\"\\nSorted Feature Importance:\")\n",
    "for i, score in sorted_features_SHAP:\n",
    "    print(f\"Feature {i}: {score:.4f}\")\n",
    "\n",
    "\n",
    "# Shapley Value Sampling\n",
    "feature_mask = torch.tensor([[0, 1, 2, 3, 4, 5, 5]]).to(DEVICE)\n",
    "svs = ShapleyValueSampling(model)\n",
    "attributions_SVS = svs.attribute(test_dataset.samples[random_indices].to(DEVICE), target=0, feature_mask=feature_mask)\n",
    "print('Scores by Shapley Value Sampling:')\n",
    "print(attributions_SVS)\n",
    "\n",
    "feature_importance_SVS = torch.mean(torch.abs(attributions_SVS), dim=0)\n",
    "sorted_features_SVS = sorted(enumerate(feature_importance_SVS.tolist()), key=lambda x: x[1], reverse=True)\n",
    "print(\"\\nSorted Feature Importance:\")\n",
    "for i, score in sorted_features_SVS:\n",
    "    print(f\"Feature {i}: {score:.4f}\")\n",
    "\n",
    "\n",
    "# DeepLIFT\n",
    "deeplift = DeepLift(model)\n",
    "attributions_deeplift = deeplift.attribute(test_dataset.samples[random_indices].to(DEVICE), target=0)\n",
    "print('Scores by DeepLIFT:')\n",
    "print(attributions_deeplift)\n",
    "\n",
    "feature_importance_deeplift = torch.mean(torch.abs(attributions_deeplift), dim=0)\n",
    "sorted_features_deeplift = sorted(enumerate(feature_importance_deeplift.tolist()), key=lambda x: x[1], reverse=True)\n",
    "print(\"\\nSorted Feature Importance:\")\n",
    "for i, score in sorted_features_deeplift:\n",
    "    print(f\"Feature {i}: {score:.4f}\")\n",
    "\n",
    "# show feature names for report\n",
    "print(\"Feature names after preprocessing:\", train_dataset.features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548f1001-218e-46a8-b4cf-303f2939c234",
   "metadata": {},
   "source": [
    "**Task 2(c)**: Perform a quantitative evaluation of the different attribution methods by computing their mean [infidelity](https://captum.ai/api/metrics.html) on the full Titanic dataset. On a high level, infidelity aims to estimate how closely the generated explanations correspond with the behaviour of the explained model by slightly perturbing the inputs and measuring how much the observed change in the model output differs from the change predicted by the corresponding feature attributions (when considering a linear model with the same weights as the feature attribution scores). If you are interested, you can find more details regarding this metric in [the original paper](https://arxiv.org/abs/1901.09392). A downside of the infidelity metric is that one needs to define a suitable perturbation function for changing the model inputs, which can significantly affect the results. In this coursework, we provide you with a perturbation function adding Gaussian noise to continuous features and performing resampling for categorical features. In your evaluation, you should experiment with two or three different standard deviations and categorical resampling probabilities. Once you are done, add a table summarising the results to your report and comment on the findings. Note that lower infidelity scores are better.\n",
    "\n",
    "Note: You should use `normalize=True` and `n_perturb_samples=10` as parameters to the Captum's infidelity function and set the same Torch and NumPy seeds before computing the infidelity for each method (so that all methods are evaluated using the same sample perturbations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffeb0f4-180f-4c7c-8a3f-258d3725ddfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from captum.metrics import infidelity, infidelity_perturb_func_decorator\n",
    "\n",
    "def perturb_func_constructor(noise_scale, cat_resample_proba, background_dataset, feature_ids, n_perturb_samples=10):\n",
    "    \"\"\"\n",
    "    You can call this function to construct a perturbation function with the desired parameters,\n",
    "    which can then be provided as the perturb_func parameter to the infidelity metric implementation\n",
    "    from Captum.\n",
    "\n",
    "     Parameters:\n",
    "        noise_scale (float): A standard deviation of the Gaussian noise added to the continuous features.\n",
    "        cat_resample_proba (float): Probability of resampling a categorical feature.\n",
    "        background_dataset (Tensor): A tensor of background data samples with the shape (num_samples, num_features).\n",
    "        feature_ids (list): A list with feature IDs, same as in generate_coalitions.\n",
    "        n_perturb_samples (int): The number of perturbed samples for each input. Should match the value\n",
    "            of the corresponding parameter to the Captum's infidelity function.\n",
    "\n",
    "    Returns:\n",
    "        perturb_func (function): A perturbation function compatible with Captum\n",
    "    \"\"\"\n",
    "    @infidelity_perturb_func_decorator(True)\n",
    "    def perturb_func(inputs):        \n",
    "        # Construct masks for noise and resampling categorical variables\n",
    "        noise_mask = torch.ones(1, inputs.size(1)).to(DEVICE)\n",
    "        # We assume that categorical features are one-hot-encoded\n",
    "        i = 0\n",
    "        current_span_start = 0\n",
    "        categorical_spans = []\n",
    "        while i < len(feature_ids) - 1:    \n",
    "            if feature_ids[i] != feature_ids[i + 1] and current_span_start != i:\n",
    "                categorical_spans.append((current_span_start, i))\n",
    "                current_span_start = i + 1\n",
    "            elif feature_ids[i] != feature_ids[i + 1]:\n",
    "                current_span_start = i + 1\n",
    "            elif feature_ids[i] == feature_ids[i + 1] and i == len(feature_ids) - 2:\n",
    "                categorical_spans.append((current_span_start, i + 1))\n",
    "            i += 1\n",
    "                \n",
    "        cat_resample_masks = []\n",
    "        for i, (s, e) in enumerate(categorical_spans):\n",
    "            cat_resample_mask = torch.zeros(inputs.shape).to(DEVICE)\n",
    "            probabilities = torch.full((inputs.size(0), 1), cat_resample_proba)\n",
    "            resample_tensor = torch.bernoulli(probabilities)\n",
    "            noise_mask[:, s:e] = 0.\n",
    "            cat_resample_mask[:, s:e] = resample_tensor\n",
    "            cat_resample_masks.append(cat_resample_mask)\n",
    "\n",
    "        # Add noise to continuous features only\n",
    "        noise = torch.tensor(np.random.normal(0, noise_scale, inputs.shape)).float().to(DEVICE) * noise_mask\n",
    "        perturbed_inputs = inputs - noise\n",
    "\n",
    "        # Randomly resample categorical variables\n",
    "        if categorical_spans:\n",
    "            expanded_background_dataset = background_dataset.repeat((n_perturb_samples, 1))\n",
    "            for cat_resample_mask in cat_resample_masks:\n",
    "                random_perm = torch.randperm(expanded_background_dataset.size(0))\n",
    "                random_samples = expanded_background_dataset[random_perm[:inputs.size(0)]]\n",
    "                perturbed_inputs = perturbed_inputs * (1 - cat_resample_mask) + random_samples * cat_resample_mask\n",
    "\n",
    "        return perturbed_inputs\n",
    "\n",
    "    return perturb_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36cb06b-f3ae-4888-905f-54d8cb5ae7bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        noise=0.05, resample=0.1  noise=0.05, resample=0.5  \\\n",
      "SHAP                                    0.041132                  0.047009   \n",
      "Shapley Value Sampling                  0.043969                  0.051140   \n",
      "DeepLift                                0.038025                  0.052279   \n",
      "\n",
      "                        noise=0.2, resample=0.1  noise=0.2, resample=0.5  \\\n",
      "SHAP                                   0.115369                 0.132789   \n",
      "Shapley Value Sampling                 0.115901                 0.127481   \n",
      "DeepLift                               0.115755                 0.133699   \n",
      "\n",
      "                        Mean Infidelity Score  \n",
      "SHAP                                 0.084075  \n",
      "Shapley Value Sampling               0.084623  \n",
      "DeepLift                             0.084940  \n",
      "Best Method with Lowest Mean Infidelity Score: SHAP\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "# Make the sampling deterministic\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define hyperparameter grid\n",
    "param_grid = {\n",
    "    'noise_scale': [0.05, 0.2],\n",
    "    'cat_resample_proba': [0.1, 0.5]\n",
    "}\n",
    "\n",
    "# Initialize infidelity \n",
    "infidelity_results = {'SHAP': [], 'Shapley Value Sampling': [], 'DeepLift': []}\n",
    "\n",
    "# Iterate over hyperparameter combinations\n",
    "for noise in param_grid['noise_scale']:\n",
    "    for prob in param_grid['cat_resample_proba']:\n",
    "        perturb_func = perturb_func_constructor(noise, prob, background_dataset=test_dataset.samples, feature_ids=[0, 1, 2, 3, 4, 5, 5], n_perturb_samples=10)\n",
    "\n",
    "        # Compute infidelity for SHAP\n",
    "        attributions_SHAP_full = shap_attribute(model, test_dataset.samples, background_dataset=test_dataset.samples, feature_ids=[0, 1, 2, 3, 4, 5, 5], target_idx=0)\n",
    "        infid_SHAP = infidelity(model, perturb_func, test_dataset.samples, attributions_SHAP_full, n_perturb_samples=10, normalize=True)\n",
    "        infidelity_results['SHAP'].append(infid_SHAP.mean().item())  # Take mean over test set\n",
    "\n",
    "        # Compute infidelity for Shapley Value Sampling\n",
    "        feature_mask = torch.tensor([[0, 1, 2, 3, 4, 5, 5]]).to(DEVICE)\n",
    "        svs_full = ShapleyValueSampling(model)\n",
    "        attributions_SVS_full = svs_full.attribute(test_dataset.samples.to(DEVICE), target=0, feature_mask=feature_mask)\n",
    "        infid_SVS = infidelity(model, perturb_func, test_dataset.samples, attributions_SVS_full, n_perturb_samples=10, normalize=True)\n",
    "        infidelity_results['Shapley Value Sampling'].append(infid_SVS.mean().item())  # Mean over test set\n",
    "\n",
    "        # Compute infidelity for DeepLIFT\n",
    "        deeplift_full = DeepLift(model)\n",
    "        attributions_deeplift_full = deeplift_full.attribute(test_dataset.samples.to(DEVICE), target=0)\n",
    "        infid_DeepLift = infidelity(model, perturb_func, test_dataset.samples, attributions_deeplift_full, n_perturb_samples=10, normalize=True) \n",
    "        infidelity_results['DeepLift'].append(infid_DeepLift.mean().item())  # Mean over test set\n",
    "\n",
    "# Compute mean infidelity across all parameter settings\n",
    "mean_infidelity = {method: sum(scores)/len(scores) for method, scores in infidelity_results.items()}\n",
    "\n",
    "# Put results into table\n",
    "param_combinations = list(product(param_grid['noise_scale'], param_grid['cat_resample_proba']))\n",
    "column_names = [f\"noise={noise}, resample={resample}\" for noise, resample in param_combinations]\n",
    "infidelity_df = pd.DataFrame.from_dict(infidelity_results, orient=\"index\", columns=column_names)\n",
    "infidelity_df[\"Mean Infidelity Score\"] = infidelity_df.mean(axis=1)\n",
    "print(infidelity_df)\n",
    "\n",
    "# Identify the best method with the lowest mean infidelity score\n",
    "best_method = min(mean_infidelity, key=mean_infidelity.get)\n",
    "print(f\"Best Method with Lowest Mean Infidelity Score: {best_method}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611be864-83ac-467c-8925-e5dfa6927463",
   "metadata": {},
   "source": [
    "**Task 2(d)**: Evaluate the computational efficiency of the different methods by taking the following steps: <br />\n",
    "**(i)** Preproccess the [Dry Bean Dataset](https://archive.ics.uci.edu/dataset/602/dry+bean+dataset), similarly to what we have done for Titanic. You can find the description of the different features on the dataset webpage along with the instructions on how to import the data in a Python environment. You do not need to perform any exploratory data analysis for this dataset. <br />\n",
    "**(ii)** Train an additional neural model on the preprocessed data. Briefly report the key performance metrics for the model in your report. <br />\n",
    "**(iii)** Compute the runtimes required to produce the attribution scores for the different methods when considering the first 200 samples in the Titanic and Dry Bean test sets. Report the results in a table in your report. Which methods seem to be the most/least computationally efficient?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4964970",
   "metadata": {},
   "source": [
    "Part (i) Import Dry Bean dataset and Preprocess it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d411bc2f-3ece-4352-8e2f-66c771e5916e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ucimlrepo in d:\\mscai\\ethics\\explainability-cw\\xaivenv\\lib\\site-packages (0.0.7)\n",
      "Requirement already satisfied: pandas>=1.0.0 in d:\\mscai\\ethics\\explainability-cw\\xaivenv\\lib\\site-packages (from ucimlrepo) (2.2.0)\n",
      "Requirement already satisfied: certifi>=2020.12.5 in d:\\mscai\\ethics\\explainability-cw\\xaivenv\\lib\\site-packages (from ucimlrepo) (2025.1.31)\n",
      "Requirement already satisfied: numpy<2,>=1.26.0 in d:\\mscai\\ethics\\explainability-cw\\xaivenv\\lib\\site-packages (from pandas>=1.0.0->ucimlrepo) (1.26.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\mscai\\ethics\\explainability-cw\\xaivenv\\lib\\site-packages (from pandas>=1.0.0->ucimlrepo) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\mscai\\ethics\\explainability-cw\\xaivenv\\lib\\site-packages (from pandas>=1.0.0->ucimlrepo) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\mscai\\ethics\\explainability-cw\\xaivenv\\lib\\site-packages (from pandas>=1.0.0->ucimlrepo) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in d:\\mscai\\ethics\\explainability-cw\\xaivenv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=1.0.0->ucimlrepo) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install ucimlrepo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db79312c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'uci_id': 602, 'name': 'Dry Bean', 'repository_url': 'https://archive.ics.uci.edu/dataset/602/dry+bean+dataset', 'data_url': 'https://archive.ics.uci.edu/static/public/602/data.csv', 'abstract': 'Images of 13,611 grains of 7 different registered dry beans were taken with a high-resolution camera. A total of 16 features; 12 dimensions and 4 shape forms, were obtained from the grains.', 'area': 'Biology', 'tasks': ['Classification'], 'characteristics': ['Multivariate'], 'num_instances': 13611, 'num_features': 16, 'feature_types': ['Integer', 'Real'], 'demographics': [], 'target_col': ['Class'], 'index_col': None, 'has_missing_values': 'no', 'missing_values_symbol': None, 'year_of_dataset_creation': 2020, 'last_updated': 'Thu Mar 28 2024', 'dataset_doi': '10.24432/C50S4B', 'creators': [], 'intro_paper': {'ID': 244, 'type': 'NATIVE', 'title': 'Multiclass classification of dry beans using computer vision and machine learning techniques', 'authors': 'M. Koklu, Ilker Ali Ã–zkan', 'venue': 'Computers and Electronics in Agriculture', 'year': 2020, 'journal': None, 'DOI': '10.1016/j.compag.2020.105507', 'URL': 'https://www.semanticscholar.org/paper/e84c31138f2f261d15517d6b6bb8922c3fe597a1', 'sha': None, 'corpus': None, 'arxiv': None, 'mag': None, 'acl': None, 'pmid': None, 'pmcid': None}, 'additional_info': {'summary': 'Seven different types of dry beans were used in this research, taking into account the features such as form, shape, type, and structure by the market situation. A computer vision system was developed to distinguish seven different registered varieties of dry beans with similar features in order to obtain uniform seed classification. For the classification model, images of 13,611 grains of 7 different registered dry beans were taken with a high-resolution camera. Bean images obtained by computer vision system were subjected to segmentation and feature extraction stages, and a total of 16 features; 12 dimensions and 4 shape forms, were obtained from the grains.', 'purpose': None, 'funded_by': None, 'instances_represent': None, 'recommended_data_splits': None, 'sensitive_data': None, 'preprocessing_description': None, 'variable_info': '1.) Area (A): The area of a bean zone and the number of pixels within its boundaries.\\r\\n2.) Perimeter (P): Bean circumference is defined as the length of its border.\\r\\n3.) Major axis length (L): The distance between the ends of the longest line that can be drawn from a bean.\\r\\n4.) Minor axis length (l): The longest line that can be drawn from the bean while standing perpendicular to the main axis.\\r\\n5.) Aspect ratio (K): Defines the relationship between L and l.\\r\\n6.) Eccentricity (Ec): Eccentricity of the ellipse having the same moments as the region.\\r\\n7.) Convex area (C): Number of pixels in the smallest convex polygon that can contain the area of a bean seed.\\r\\n8.) Equivalent diameter (Ed): The diameter of a circle having the same area as a bean seed area.\\r\\n9.) Extent (Ex): The ratio of the pixels in the bounding box to the bean area.\\r\\n10.)Solidity (S): Also known as convexity. The ratio of the pixels in the convex shell to those found in beans.\\r\\n11.)Roundness (R): Calculated with the following formula: (4piA)/(P^2)\\r\\n12.)Compactness (CO): Measures the roundness of an object: Ed/L\\r\\n13.)ShapeFactor1 (SF1)\\r\\n14.)ShapeFactor2 (SF2)\\r\\n15.)ShapeFactor3 (SF3)\\r\\n16.)ShapeFactor4 (SF4)\\r\\n17.)Class (Seker, Barbunya, Bombay, Cali, Dermosan, Horoz and Sira)\\r\\n', 'citation': None}}\n",
      "               name     role         type demographic  \\\n",
      "0              Area  Feature      Integer        None   \n",
      "1         Perimeter  Feature   Continuous        None   \n",
      "2   MajorAxisLength  Feature   Continuous        None   \n",
      "3   MinorAxisLength  Feature   Continuous        None   \n",
      "4       AspectRatio  Feature   Continuous        None   \n",
      "5      Eccentricity  Feature   Continuous        None   \n",
      "6        ConvexArea  Feature      Integer        None   \n",
      "7     EquivDiameter  Feature   Continuous        None   \n",
      "8            Extent  Feature   Continuous        None   \n",
      "9          Solidity  Feature   Continuous        None   \n",
      "10        Roundness  Feature   Continuous        None   \n",
      "11      Compactness  Feature   Continuous        None   \n",
      "12     ShapeFactor1  Feature   Continuous        None   \n",
      "13     ShapeFactor2  Feature   Continuous        None   \n",
      "14     ShapeFactor3  Feature   Continuous        None   \n",
      "15     ShapeFactor4  Feature   Continuous        None   \n",
      "16            Class   Target  Categorical        None   \n",
      "\n",
      "                                          description   units missing_values  \n",
      "0   The area of a bean zone and the number of pixe...  pixels             no  \n",
      "1   Bean circumference is defined as the length of...    None             no  \n",
      "2   The distance between the ends of the longest l...    None             no  \n",
      "3   The longest line that can be drawn from the be...    None             no  \n",
      "4   Defines the relationship between MajorAxisLeng...    None             no  \n",
      "5   Eccentricity of the ellipse having the same mo...    None             no  \n",
      "6   Number of pixels in the smallest convex polygo...    None             no  \n",
      "7   Equivalent diameter: The diameter of a circle ...    None             no  \n",
      "8   The ratio of the pixels in the bounding box to...    None             no  \n",
      "9   Also known as convexity. The ratio of the pixe...    None             no  \n",
      "10  Calculated with the following formula: (4piA)/...    None             no  \n",
      "11                Measures the roundness of an object    Ed/L             no  \n",
      "12                                               None    None             no  \n",
      "13                                               None    None             no  \n",
      "14                                               None    None             no  \n",
      "15                                               None    None             no  \n",
      "16  (Seker, Barbunya, Bombay, Cali, Dermosan, Horo...    None             no  \n"
     ]
    }
   ],
   "source": [
    "# Import Dry Bean dataset into code\n",
    "from ucimlrepo import fetch_ucirepo \n",
    "\n",
    "# fetch dataset \n",
    "dry_bean = fetch_ucirepo(id=602) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X_drybean = dry_bean.data.features \n",
    "y_drybean = dry_bean.data.targets \n",
    "\n",
    "# metadata \n",
    "print(dry_bean.metadata) \n",
    "\n",
    "# variable information \n",
    "print(dry_bean.variables) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff46d585",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\MScAI\\ethics\\explainability-cw\\xaivenv\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:114: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "d:\\MScAI\\ethics\\explainability-cw\\xaivenv\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:132: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "# Create train test split\n",
    "x_train_drybean, x_test_drybean, y_train_drybean, y_test_drybean = train_test_split(X_drybean, y_drybean, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "# Preprocess the data\n",
    "x_train_processed_drybean, preprocessor_drybean = preprocess_train_data(x_train_drybean, scaled_features=x_train_drybean.columns)\n",
    "x_test_processed_drybean = preprocess_test_data(x_test_drybean, preprocessor_drybean)\n",
    "x_train_drybean = pd.DataFrame(x_train_processed_drybean, columns=preprocessor_drybean.get_feature_names_out())\n",
    "x_test_drybean = pd.DataFrame(x_test_processed_drybean, columns=preprocessor_drybean.get_feature_names_out())\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_drybean = label_encoder.fit_transform(y_train_drybean)\n",
    "y_test_drybean = label_encoder.transform(y_test_drybean)\n",
    "\n",
    "# Convert to tensors\n",
    "x_train_drybean = torch.tensor(x_train_drybean.to_numpy(), dtype=torch.float32)\n",
    "x_test_drybean = torch.tensor(x_test_drybean.to_numpy(), dtype=torch.float32)\n",
    "y_train_drybean = torch.tensor(y_train_drybean, dtype=torch.long)\n",
    "y_test_drybean = torch.tensor(y_test_drybean, dtype=torch.long)\n",
    "\n",
    "# Create PyTorch Dataset for preprocessed tensors\n",
    "train_dataset_drybean = TensorDataset(x_train_drybean, y_train_drybean)\n",
    "test_dataset_drybean = TensorDataset(x_test_drybean, y_test_drybean)\n",
    "\n",
    "# Define DataLoaders_\n",
    "train_dl_drybean = DataLoader(\n",
    "    dataset=train_dataset_drybean,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    ")\n",
    "test_dl_drybean = DataLoader(\n",
    "    dataset=test_dataset_drybean,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e1c180",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torcheval.metrics.functional import multiclass_f1_score, multiclass_accuracy\n",
    "def construct_nn(nn_dims, activation_fun):\n",
    "    \"\"\"\n",
    "    Constructs a neural network for multi-class classification.\n",
    "    \"\"\"\n",
    "    layers = []\n",
    "    for i in range(1, len(nn_dims)):\n",
    "        in_dim, out_dim = nn_dims[i-1], nn_dims[i]\n",
    "        layers.append(nn.Linear(in_dim, out_dim))\n",
    "        if i < len(nn_dims) - 1:  # Apply activation except for the last layer\n",
    "            layers.append(activation_fun())\n",
    "\n",
    "    # Add Softmax activation for multi-class classification\n",
    "    layers.append(nn.Softmax(dim=1))\n",
    "\n",
    "    return nn.Sequential(*layers).to(DEVICE)\n",
    "\n",
    "\n",
    "def train_nn(model, train_dl_drybean, num_epochs=100):\n",
    "    \"\"\"\n",
    "    Trains a neural network using the data from the provided data loader.\n",
    "    \"\"\"\n",
    "    loss_fun = nn.CrossEntropyLoss()\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "    model.train()\n",
    "    \n",
    "    losses = []\n",
    "    for epoch in tqdm(range(num_epochs), leave=False):\n",
    "        total_loss = 0\n",
    "        for i, (x, y) in list(enumerate(train_dl_drybean)):\n",
    "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "            opt.zero_grad()\n",
    "            out = model(x)\n",
    "            loss = loss_fun(out, y)\n",
    "            total_loss += loss.item()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "        losses.append(total_loss)\n",
    "\n",
    "def eval_nn(model, test_dl, num_classes=7, class_names=['Seker', 'Barbunya', 'Bombay', 'Cali', 'Dermosan', 'Horoz', 'Sira']):\n",
    "    \"\"\"\n",
    "    Evaluates multi-class classification performance of a model on the given\n",
    "    test dataset.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    loss_fun = nn.CrossEntropyLoss()\n",
    "\n",
    "    x_test, y_test = test_dataset_drybean.tensors\n",
    "    x_test, y_test = x_test.to(DEVICE), y_test.to(DEVICE)\n",
    "\n",
    "    predictions = model(x_test)\n",
    "    loss = loss_fun(predictions, y_test).item()\n",
    "\n",
    "    predicted_classes = torch.argmax(predictions, dim=1).detach()\n",
    "    labels = y_test.detach()\n",
    "\n",
    "    macro_f1 = multiclass_f1_score(predicted_classes, labels, num_classes=len(class_names), average=\"macro\").item()\n",
    "    accuracy = multiclass_accuracy(predicted_classes, labels, num_classes=len(class_names)).item()\n",
    "    per_class_f1 = multiclass_f1_score(predicted_classes, labels, num_classes=len(class_names), average=None)\n",
    "    \n",
    "    print(\"\\n**Per-Class F1 Scores:**\")\n",
    "    for i, score in enumerate(per_class_f1.tolist()):\n",
    "        print(f\"  - {class_names[i]}: {score:.4f}\")\n",
    "\n",
    "    return loss, macro_f1, per_class_f1, accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c0fd09",
   "metadata": {},
   "source": [
    "Part (ii) Train an additional neural model on the preprocessed data. Briefly report the key performance metrics for the model in your report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e5e2d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â€”â€”â€”â€”â€”â€”â€”[ Model training ]â€”â€”â€”â€”â€”â€”â€”\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "703a6742554b49dda4d209304b6ffa3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed!\n",
      "\n",
      "â€”â€”â€”â€”â€”â€”â€”[ Evaluation ]â€”â€”â€”â€”â€”â€”â€”\n",
      "\n",
      "**Per-Class F1 Scores:**\n",
      "  - Seker: 0.9553\n",
      "  - Barbunya: 1.0000\n",
      "  - Bombay: 0.9503\n",
      "  - Cali: 0.9099\n",
      "  - Dermosan: 0.9554\n",
      "  - Horoz: 0.9474\n",
      "  - Sira: 0.8805\n",
      "Loss: 1.23\n",
      "Macro F1 score: 0.94\n",
      "Accuracy: 0.93\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Construct model for Dry Bean dataset\n",
    "model_drybean = construct_nn([16, 128, 128, 7], nn.ReLU).to(DEVICE)\n",
    "\n",
    "print(\"â€”â€”â€”â€”â€”â€”â€”[ Model training ]â€”â€”â€”â€”â€”â€”â€”\")\n",
    "train_nn(model_drybean, train_dl_drybean, num_epochs=1000)\n",
    "print(\"Training completed!\")\n",
    "print()\n",
    "\n",
    "\n",
    "print(\"â€”â€”â€”â€”â€”â€”â€”[ Evaluation ]â€”â€”â€”â€”â€”â€”â€”\")\n",
    "test_loss, macro_f1, per_class_f1, accuracy = eval_nn(model_drybean, test_dataset_drybean)\n",
    "print_metric(\"Loss\", test_loss)\n",
    "print_metric(\"Macro F1 score\", macro_f1)\n",
    "print_metric(\"Accuracy\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e587a4c",
   "metadata": {},
   "source": [
    "Part (iii) Compute the runtimes required to produce the attribution scores for the different methods when considering the first 200 samples in the Titanic and Dry Bean test sets. Report the results in a table in your report. Which methods seem to be the most/least computationally efficient?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233a8bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime for titanic dataset with SVS: 0.12891888618469238\n",
      "Runtime for drybean dataset with SVS: 0.3229794502258301\n",
      "Runtime for titanic dataset with DeepLift: 0.012480497360229492\n",
      "Runtime for drybean dataset with DeepLift: 0.017811298370361328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\MScAI\\ethics\\explainability-cw\\xaivenv\\Lib\\site-packages\\captum\\_utils\\gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n",
      "d:\\MScAI\\ethics\\explainability-cw\\xaivenv\\Lib\\site-packages\\captum\\attr\\_core\\deep_lift.py:304: UserWarning: Setting forward, backward hooks and attributes on non-linear\n",
      "               activations. The hooks and attributes will be removed\n",
      "            after the attribution is finished\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime for titanic dataset with SHAP: 416.5688941478729\n",
      "Run time exceeds 10 min, indicating SHAP is a lot less computationally efficient than Shapley Value Sampling and DeepLift.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import multiprocessing\n",
    "\n",
    "# First 200 samples\n",
    "titanic_samples = test_dataset.samples[:200]\n",
    "drybean_samples = test_dataset_drybean.tensors[0][:200]\n",
    "\n",
    "# Background_dataset for SHAP implementation from above\n",
    "background_indices_titanic = torch.randperm(len(test_dataset.samples))[:200]  \n",
    "background_dataset_titanic = test_dataset.samples[background_indices_titanic]\n",
    "background_indices_drybean = torch.randperm(len(test_dataset_drybean))[:200]\n",
    "background_dataset_drybean = test_dataset_drybean.tensors[0][background_indices_drybean] \n",
    "\n",
    "# Get targets for first 200 samples of drybean dataset\n",
    "drybean_targets = test_dataset_drybean.tensors[1][:200]\n",
    "\n",
    "# Shapley Value Sampling - Titanic\n",
    "feature_mask_titanic = torch.tensor([[0, 1, 2, 3, 4, 5, 5]]).to(DEVICE)\n",
    "svs_titanic = ShapleyValueSampling(model)\n",
    "start_time_titanic_svs = time.time()\n",
    "attributions_SVS_titanic = svs_titanic.attribute(titanic_samples.to(DEVICE), target=0, feature_mask=feature_mask_titanic)\n",
    "end_time_titanic_svs = time.time()\n",
    "time_titanic_svs = end_time_titanic_svs - start_time_titanic_svs\n",
    "print(f\"Runtime for titanic dataset with SVS: {time_titanic_svs}\")\n",
    "\n",
    "# Shapley Value Sampling - Drybean\n",
    "feature_mask_drybean = torch.tensor([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]]).to(DEVICE)\n",
    "svs_drybean = ShapleyValueSampling(model_drybean)\n",
    "start_time_drybean_svs = time.time()\n",
    "attributions_SVS_drybean = svs_drybean.attribute(drybean_samples.to(DEVICE), target=drybean_targets.to(DEVICE), feature_mask=feature_mask_drybean)\n",
    "end_time_drybean_svs = time.time()\n",
    "time_drybean_svs = end_time_drybean_svs - start_time_drybean_svs\n",
    "print(f\"Runtime for drybean dataset with SVS: {time_drybean_svs}\")\n",
    "\n",
    "\n",
    "# DeepLift - Titanic\n",
    "deeplift_titanic = DeepLift(model)\n",
    "start_time_titanic_deeplift = time.time()\n",
    "attributions_deeplift_titanic = deeplift_titanic.attribute(titanic_samples.to(DEVICE), target=0)\n",
    "end_time_titanic_deeplift = time.time()\n",
    "time_titanic_deeplift = end_time_titanic_deeplift - start_time_titanic_deeplift\n",
    "print(f\"Runtime for titanic dataset with DeepLift: {time_titanic_deeplift}\")\n",
    "\n",
    "# DeepLift - Drybean\n",
    "deeplift_drybean = DeepLift(model_drybean)\n",
    "start_time_drybean_deeplift = time.time()\n",
    "attributions_deeplift_drybean = deeplift_drybean.attribute(drybean_samples.to(DEVICE), target=drybean_targets.to(DEVICE))\n",
    "end_time_drybean_deeplift = time.time()\n",
    "time_drybean_deeplift = end_time_drybean_deeplift - start_time_drybean_deeplift\n",
    "print(f\"Runtime for drybean dataset with DeepLift: {time_drybean_deeplift}\")\n",
    "\n",
    "\n",
    "# SHAP implementation from above - Titanic\n",
    "start_time_titanic_SHAP = time.time()\n",
    "attributions_SHAP_titanic = shap_attribute(model, titanic_samples, background_dataset_titanic, feature_ids=[0, 1, 2, 3, 4, 5, 5], target_idx=0)\n",
    "end_time_titanic_SHAP = time.time()\n",
    "time_titanic_SHAP = end_time_titanic_SHAP - start_time_titanic_SHAP\n",
    "print(f\"Runtime for titanic dataset with SHAP: {time_titanic_SHAP}\")\n",
    "\n",
    "\n",
    "# SHAP implementation from above - Drybean\n",
    "TIMEOUT_LIMIT = 10 * 60\n",
    "\n",
    "def run_shap(model, x, background_dataset, feature_ids, target_idx, return_dict):\n",
    "    \"\"\" Function to execute SHAP and store time result in a dictionary. \"\"\"\n",
    "    #start_time = time.time()\n",
    "    attributions = shap_attribute(model, x, background_dataset, feature_ids, target_idx)\n",
    "    #end_time = time.time()\n",
    "    #return_dict[\"runtime\"] = end_time - start_time\n",
    "    return_dict[\"completed\"] = True\n",
    "\n",
    "def compute_shap_with_timeout(model, x, background_dataset, feature_ids, target_idx):\n",
    "    \"\"\" Runs SHAP and enforces a 10-minute timeout. \"\"\"\n",
    "    # Create a shared dictionary to store results\n",
    "    manager = multiprocessing.Manager()\n",
    "    return_dict = manager.dict()\n",
    "    return_dict[\"completed\"] = False\n",
    "\n",
    "    # Start SHAP computation in a separate process\n",
    "    process = multiprocessing.Process(target=run_shap, args=(model, x, background_dataset, feature_ids, target_idx, return_dict))\n",
    "    process.start()\n",
    "    process.join(timeout=TIMEOUT_LIMIT)\n",
    "\n",
    "    if process.is_alive():\n",
    "        process.terminate()\n",
    "        process.join()\n",
    "        print(\"Run time exceeds 10 min, indicating SHAP is a lot less computationally efficient than Shapley Value Sampling and DeepLift.\")\n",
    "        return None\n",
    "\n",
    "    if not return_dict[\"completed\"]:\n",
    "        print(\"Run time exceeds 10 min, indicating SHAP is a lot less computationally efficient than Shapley Value Sampling and DeepLift.\")\n",
    "        return None\n",
    "    \n",
    "    return TIMEOUT_LIMIT\n",
    "\n",
    "# Run SHAP with a timeout\n",
    "time_drybean_SHAP = compute_shap_with_timeout(\n",
    "    model_drybean, drybean_samples, background_dataset_drybean, \n",
    "    feature_ids=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], \n",
    "    target_idx=drybean_targets\n",
    ")\n",
    "\n",
    "if time_drybean_SHAP is not None:\n",
    "    print(f\"Runtime for drybean dataset with SHAP was greater than {time_drybean_SHAP:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ec783c-e350-473e-b3e1-140095ebed53",
   "metadata": {},
   "source": [
    "## Counterfactual Explanations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb92884-22cd-46e4-824d-8354e35ce856",
   "metadata": {},
   "source": [
    "### Designing a Distance Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128f2aac-ad49-4a47-a74a-21158879f06d",
   "metadata": {},
   "source": [
    "**Task 3(a)**: First, we need to specify a suitable distance metric for measuring the closeness between the points. Depending on the dataset, one could choose the standard distance functions like the Manhattan (L1) distance, Euclidean (L2) distance, and more specialised ones like Gower distance for better handling datasets with both categorical and continuous dataset. However, the design of distance metric can be very flexible. For example, the standard L1 distance is (k is the number of features) $$d_{L1}(x, x') = \\sum_{i}^{k} |x_i-x'_i|$$\n",
    "\n",
    "If the features have different value ranges, we could normalise the L1 distance with the maximum and minimum values of each feature (indexed $i$) in the training dataset, $max_i$, $min_i$ : $$d_{L1, normalised}(x, x') = \\sum_{i}^{k} |(x_i-x'_i)/(max_i-min_i)|$$\n",
    "\n",
    "On top of this, we could also add customised weighting factors $\\mathbf{w}=w_1, ..., w_k$ to capture the importance of each feature, and the weighted L1 distance is: $$d_{L1, normalised, weighted}(x, x') = \\sum_{i}^{k} w_i|(x_i-x'_i)/(max_i-min_i)|$$\n",
    "\n",
    "Given the background above, we want to design a distance function for the preprocessed version of our Titanic dataset. Explore the dataset characteristics and answer the following questions:\n",
    "**a)** Briefly discuss the weighting of each input variable in the preprocessed dataset, if we use standard L1 and normalised L1?\n",
    "**b)** If we want to treat each feature equally in the original unprocessed dataset, how would you design the distance metric for the preprocessed dataset using L1-based distance? Write down the detail of your distance function for the preprocessed dataset and justify why each original feature is treated equally.\n",
    "**c)** Implement your distance function below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "2b077fa1-b082-438a-806b-119714ca33eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_function(x1, x2):\n",
    "    \"\"\"\n",
    "    Your distance function.\n",
    "\n",
    "    Parameters:\n",
    "        x1 (Tensor): A 1-d array of shape (k,)\n",
    "        x2 (Tensor): A 1-d array of shape (k,)\n",
    "\n",
    "    Returns:\n",
    "        distance (float): A real number >= 0\n",
    "    \"\"\"\n",
    "    # Compute feature-wise min and max using only the training dataset\n",
    "    feature_min = train_dataset.samples.min(dim=0).values\n",
    "    feature_max = train_dataset.samples.max(dim=0).values\n",
    "    feature_ranges = feature_max - feature_min\n",
    "\n",
    "    # Avoid division by zero for constant features\n",
    "    feature_ranges[feature_ranges == 0] = 1\n",
    "    \n",
    "    # Compute normalising weight\n",
    "    weights = 1/feature_ranges\n",
    "\n",
    "    # Adjust weight for sex feature as it is splitted into two columns (male and female)\n",
    "    sex_feature_indices = [i for i, name in enumerate(train_dataset.features) if \"sex\" in name]\n",
    "    for idx in sex_feature_indices:\n",
    "        weights[idx] /= len(sex_feature_indices)  # Reduce weight contribution by the number of one-hot columns\n",
    "    \n",
    "    # Compute weighted L1 distance\n",
    "    weighted_diff = torch.abs(x1 - x2) * weights\n",
    "    distance = torch.sum(weighted_diff)\n",
    "\n",
    "    return distance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df345ad0-6bdb-47b9-abf9-598c23a963c4",
   "metadata": {},
   "source": [
    "### Nearest-Neighbour Counterfactual Explanations (NNCE)\n",
    "\n",
    "**Task 3(b)**: As introduced in the tutorial, NNCEs are a simple yet effective method for finding counterfactuals. Implement the NNCE functions.\n",
    "\n",
    "Instructions:\n",
    "1. determine desired label for the counterfactual\n",
    "2. find the dataset points with desired label as predicted by the model\n",
    "3. find the point with the minimum distance and return it as NNCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "47409a30-fbad-4371-99d6-b87d30cd6590",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_nnce(x, m, train_set, dist):\n",
    "    \"\"\"\n",
    "    Function to compute NNCE.\n",
    "\n",
    "    Parameters:\n",
    "        x (Tensor): Input, a 1-d array of shape (k,)\n",
    "        m (Sequential): Our neural network\n",
    "        train_set (TitanicDataset): Our Titanic dataset\n",
    "        dist (function): Your previously implemented distance function\n",
    "\n",
    "    Returns:\n",
    "        nnce (Tensor): Nearest neighbour counterfactual explanation, an 1-d array of shape (k,)\n",
    "    \"\"\"\n",
    "    # Initialise list used to store candidate counterfactuals\n",
    "    candidates = []\n",
    "\n",
    "    # Get label of input instance\n",
    "    label = torch.round(m(x.to(DEVICE))).item()\n",
    "\n",
    "    for i in range(len(train_set)):\n",
    "        xi, yi = train_set.samples[i], train_set.labels[i]\n",
    "        yi_pred = torch.round(m(xi.to(DEVICE))).item()\n",
    "\n",
    "        # Retain only inputs with required label\n",
    "        if yi_pred == 1 - label:\n",
    "            distance = dist(x, xi)\n",
    "            candidates.append((xi, distance))\n",
    "    \n",
    "    # Get the counterfactual with minimum distance\n",
    "    if candidates:\n",
    "        nnce, _ = min(candidates, key=lambda x: x[1])\n",
    "        return nnce\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "07e25d4e-eb20-4a62-a468-64ce6834d6ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0040], grad_fn=<SigmoidBackward0>)\n",
      "tensor([0.5058], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# use this code block to test if your function is working\n",
    "# first print out the original input's prediction result\n",
    "test_input = test_dataset.samples.to(DEVICE)[0]\n",
    "print(model(test_input))\n",
    "\n",
    "# now compute NNCE and print out the NNCE's prediction result. Ideally this is different from the result for the original input.\n",
    "nnce = compute_nnce(test_input, model, train_set=train_dataset, dist=distance_function)\n",
    "print(model(nnce))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c6290a-7c2b-4064-8ed0-3a4056bac9dc",
   "metadata": {},
   "source": [
    "### Gradient-Based Counterfactual Explanations\n",
    "\n",
    "**Task 3(c)**: Complete the PyTorch implementation for the gradient-based method in [Wachter et al. 2017]: WAC.\n",
    "\n",
    "Instructions:\n",
    "1. We are going to optimise the following loss function to find a counterfactual x': $  argmin_{x'} \\text{ } BCE(y', (1-y)) + \\lambda cost(x, x')$, where $BCE$ is binary cross entropy loss, $y'$ is the predicted label of $x'$, $y$ is the predicted label of $x$, $cost(,)$ is your chosen distance function in Task 1, and $\\lambda$ is the trade-off parameter between validity and proximity. First, implement your chosen distance metric in ```CostLoss.forward()```\n",
    "2. Follow the code structure in the ```compute_wac()``` function, complete the implementation.\n",
    "    2.1. specify the target label for the counterfactual\n",
    "    2.2. implement gradient descent procedures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "aa6aae81-d6bb-4037-810d-198c00143334",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class CostLoss(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(CostLoss, self).__init__()\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        \"\"\"\n",
    "        The PyTorch version of your distance function\n",
    "\n",
    "        Parameters:\n",
    "            x1 (Tensor): A 1-d array of shape (k,)\n",
    "            x2 (Tensor): A 1-d array of shape (k,)\n",
    "\n",
    "        Returns:\n",
    "            distance (Tensor): a real number\n",
    "        \"\"\"\n",
    "        # TODO: Here is an example of standard L1 loss, replace it with your designed distance function in Task 1\n",
    "        dist = distance_function(x1, x2)\n",
    "        return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "eb8ae1f7-8c47-4a71-a817-2f22d48a64b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "from torch.optim import Adam\n",
    "import datetime\n",
    "\n",
    "def compute_wac(x, m, lamb=0.1, lr=0.01, max_iter=1000, max_allowed_minutes=0.5):\n",
    "    \"\"\"\n",
    "    Function to find WAC using gradient descent.\n",
    "\n",
    "    Parameters:\n",
    "        x (Tensor): Input x, an 1-d array of shape (k,)\n",
    "        m (Sequential): PyTorch model\n",
    "        lamb (float): Lambda, the tradeoff term in the loss function\n",
    "        lr (float): Learning rate for gradient descent\n",
    "        max_iter (int): maximum allowed iteration\n",
    "        max_allowed_minutes (float): maximum allowed minutes\n",
    "\n",
    "    Returns:\n",
    "        counterfactual (Tensor): Counterfactual point, an 1-d array of shape (k,)\n",
    "    \"\"\"\n",
    "    # initialise the counterfactual search at the input point\n",
    "    x = x.to(DEVICE)\n",
    "    wac = Variable(x.clone(), requires_grad=True).to(DEVICE)\n",
    "\n",
    "    # initialise an optimiser for gradient descent over the wac counterfactual point\n",
    "    optimiser = Adam([wac], lr, amsgrad=True)\n",
    "\n",
    "    # instantiate the two components of the loss function\n",
    "    validity_loss = torch.nn.BCELoss()\n",
    "    cost_loss = CostLoss()\n",
    "\n",
    "    # TASK: specify target label y: either 0 or 1, depending on the original prediction\n",
    "    # TODO: Start your code here\n",
    "    y_target = torch.tensor([1 - torch.round(m(x)).item()], dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "    # the total loss in the instructions: loss = validity_loss + lamb * cost_loss\n",
    "    # TODO: End your code here\n",
    "    loss = validity_loss(m(wac), y_target) + lamb * cost_loss(x, wac)\n",
    "\n",
    "    # compute class probability\n",
    "    class_prob = m(wac)\n",
    "    wac_valid = False\n",
    "    iterations = 0\n",
    "    if y_target == 0 and class_prob < 0.5 or y_target == 1 and class_prob >= 0.5:\n",
    "        wac_valid = True\n",
    "\n",
    "    # set maximum allowed time for computing 1 counterfactual\n",
    "    t0 = datetime.datetime.now()\n",
    "    t_max = datetime.timedelta(minutes=max_allowed_minutes)\n",
    "    # start gradient descent\n",
    "    while not wac_valid or iterations <= max_iter:\n",
    "\n",
    "        # TASK: gradient descent to find wac\n",
    "        # TODO: Your code here\n",
    "\n",
    "        # Zero gradient for optimiser to avoid gradient accumulating\n",
    "        optimiser.zero_grad()\n",
    "        \n",
    "        # Loss is calculated above\n",
    "        loss = validity_loss(m(wac), y_target) + lamb * cost_loss(x, wac)\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient Descent\n",
    "        optimiser.step()\n",
    "        \n",
    "        # Update class probability\n",
    "        class_prob = m(wac)\n",
    "        \n",
    "        # break conditions: valid counterfactual found, or iterations exceeded, or reached allowed max time\n",
    "        if y_target == 0 and class_prob < 0.5 or y_target == 1 and class_prob >= 0.5:\n",
    "            wac_valid = True\n",
    "        if datetime.datetime.now() - t0 > t_max:\n",
    "            break\n",
    "        iterations += 1\n",
    "\n",
    "    return wac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "3730ddc2-6dbe-46a2-8bf3-e833800c52ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0080], grad_fn=<SigmoidBackward0>)\n",
      "tensor([0.9965], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# use this code block to test if your function is working\n",
    "# first print out the original input's prediction result\n",
    "test_input = test_dataset.samples.to(DEVICE)[1]\n",
    "print(model(test_input))\n",
    "\n",
    "# now compute WAC and print out the WAC's prediction result. Ideally this is different from the result for the original input.\n",
    "nnce = compute_wac(test_input, model)\n",
    "print(model(nnce))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb7d395-b999-4100-85dd-7b5eed5c57d7",
   "metadata": {},
   "source": [
    "### Performance of the Two Methods\n",
    "\n",
    "**Task 3(d)**: In order to understand better how these two methods compare, we use the following metrics to quantitatively evaluate each of the methods:\n",
    "- Validity: percentage of the counterfactuals that are valid.\n",
    "- Proximity: average distance between the counterfactuals and the inputs. Smaller distance (lower cost) indicates better proximity.\n",
    "- Plausibility: average distance of a counterfactual to its 5 nearest neighbours in the training dataset, further averaged over all counterfactuals. The closer it is to the nearest neighbours, the more plausible. Consider this metric as a simplified version of Local Outlier Factor.\n",
    "\n",
    "For each counterfactual method, we randomly select 20 test inputs, generate counterfactuals for them, and compare the average performances for each of the metrics.\n",
    "We repeat this process for 5 times and calculate the mean and standard deviation of each metrics. We have provided code for these experiments. Complete the following codes for calculating the evaluation metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "4847e8d0-c76c-462d-a986-ccd2dc300e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_three_metrics_for_group_of_inputs(inputs, m, counterfactuals, train_set, dist):\n",
    "    validity, proximity, plausibility = 0, 0, 0\n",
    "    # examine validity, proximity, plausibility for each input-counterfactual pair\n",
    "    for i, x in enumerate(inputs):\n",
    "        ce = counterfactuals[i]\n",
    "\n",
    "        this_val = calculate_validity(x, ce, m)\n",
    "        this_prox = calculate_proximity(x, ce, dist)\n",
    "        this_plaus = calculate_plausibility(ce, train_set, dist)\n",
    "\n",
    "        validity += this_val\n",
    "        proximity += this_prox\n",
    "        plausibility += this_plaus\n",
    "\n",
    "    # average evaluation metrics over all the test inputs\n",
    "    validity = validity / len(inputs)\n",
    "    proximity = proximity / len(inputs)\n",
    "    plausibility = plausibility / len(inputs)\n",
    "    return float(validity), float(proximity), float(plausibility)\n",
    "\n",
    "\n",
    "# TASK: for each input-counterfactual pair, calculate validity, proximity, and plausibility\n",
    "\n",
    "# check whether a counterfactual ce is valid or not\n",
    "def calculate_validity(x, ce, m):\n",
    "    # TODO: Your code here\n",
    "    #return m(ce) == 1 - m(x)\n",
    "    return float(torch.round(m(ce)) == 1 - torch.round(m(x)))\n",
    "\n",
    "\n",
    "# check the distance between a counterfactual and the input\n",
    "def calculate_proximity(x, ce, dist):\n",
    "    # TODO: Your code here\n",
    "    return dist(x, ce)\n",
    "\n",
    "# calculate plausibility of a counterfactual\n",
    "def calculate_plausibility(ce, train_set, dist):\n",
    "    # here, calculate the average distance between the counterfactual and its 5 nearest neighbours in the training dataset\n",
    "    \n",
    "    # TODO: Your code here\n",
    "    neighbours_and_distance = [(x, dist(x, ce)) for x in train_set.samples]\n",
    "    sorted_neighbours = sorted(neighbours_and_distance, key=lambda x: x[1])\n",
    "    closest_5_neighbours = sorted_neighbours[:5]\n",
    "    avg_distance = sum(point[1] for point in closest_5_neighbours)/5\n",
    "    return avg_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1cb9d3-cb0b-4473-b7d2-166e76032c61",
   "metadata": {},
   "source": [
    "Now we set up the experiments, compute counterfactuals using NNCE and WAC, then evaluate and compare their performances. Note that depending on your machine, the computation for ```compute_wac()``` could potentially be slow. You can also manually change the function's hyperparameters ```lamb=0.1, lr=0.01, max_iter=1000``` to try and see if WAC could give better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "9d10f578-a107-4fc5-8a33-246018d9b6c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2772a7e502164cdb830d1fe94e72330f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15f409590ff14af7969a97a0d6690531",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f9582d97459435283f8ba601eae40f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9423e26e13e34b369e673e5afaec1ca6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "068c4386039f4fb690cfe863752ed31d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fda01eec509d4a639cb24cfc675d85cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce4e9a7e8c274636bb8454b5ae9e1dd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b3ee7cd82c447d5ba4b44ad7f1e5cd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "885c8c58140d42ef83e90307c912500d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca1909b89afc4b71a178c21353d85266",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ed76c210c024c9a8a8b525de2ab9cb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+----------------+----------------+\n",
      "| method   | validity      | cost           | plausibility   |\n",
      "+==========+===============+================+================+\n",
      "| NNCE     | 1.0 +- 0.0    | 0.139 +- 0.018 | 0.032 +- 0.004 |\n",
      "| WAC      | 0.88 +- 0.075 | 0.194 +- 0.027 | 0.197 +- 0.027 |\n",
      "+----------+---------------+----------------+----------------+\n"
     ]
    }
   ],
   "source": [
    "# we store the evaluation results of the five runs in lists\n",
    "nnce_validity, nnce_proximity, nnce_plausibility = [], [], []\n",
    "wac_validity, wac_proximity, wac_plausibility = [], [], []\n",
    "seed_num = 1000\n",
    "\n",
    "# repeat over 5 runs to obtain more robust evaluations\n",
    "for one_run in tqdm(range(5)):\n",
    "    # randomly select 20 test inputs\n",
    "    np.random.seed(seed_num)\n",
    "    test_inputs = test_dataset.samples[np.random.choice(range(len(test_dataset.samples)), 20)]\n",
    "    nnce_counterfactuals, wac_counterfactuals = [], []\n",
    "    # generate counterfactuals\n",
    "    with tqdm(total=1, position=0, leave=True) as pbar:\n",
    "        for x in tqdm(test_inputs, position=0, leave=True):\n",
    "            nnce_counterfactuals.append(compute_nnce(x, model, train_dataset, distance_function))\n",
    "            wac_counterfactuals.append(compute_wac(x, model, lamb=0.1))\n",
    "            pbar.update()\n",
    "\n",
    "    # evaluate counterfactuals\n",
    "    nnvalidity, nnproximity, nnplausibility = calculate_three_metrics_for_group_of_inputs(test_inputs, model,\n",
    "                                                                                          nnce_counterfactuals,\n",
    "                                                                                          train_dataset,\n",
    "                                                                                          distance_function)\n",
    "    nnce_validity.append(nnvalidity)\n",
    "    nnce_proximity.append(nnproximity)\n",
    "    nnce_plausibility.append(nnplausibility)\n",
    "\n",
    "    wvalidity, wproximity, wplausibility = calculate_three_metrics_for_group_of_inputs(test_inputs, model,\n",
    "                                                                                       wac_counterfactuals,\n",
    "                                                                                       train_dataset, distance_function)\n",
    "    wac_validity.append(wvalidity)\n",
    "    wac_proximity.append(wproximity)\n",
    "    wac_plausibility.append(wplausibility)\n",
    "\n",
    "    seed_num += 10\n",
    "\n",
    "# now print out the results\n",
    "from tabulate import tabulate\n",
    "\n",
    "score_names = [\"method\", \"validity\", \"cost\", \"plausibility\"]\n",
    "score_table = [score_names,\n",
    "               [\"NNCE\", f\"{np.mean(nnce_validity).round(3)} +- {np.std(nnce_validity).round(3)}\",\n",
    "                f\"{np.mean(nnce_proximity).round(3)} +- {np.std(nnce_proximity).round(3)}\",\n",
    "                f\"{np.mean(nnce_plausibility).round(3)} +- {np.std(nnce_plausibility).round(3)}\"],\n",
    "               [\"WAC\", f\"{np.mean(wac_validity).round(3)} +- {np.std(wac_validity).round(3)}\",\n",
    "                f\"{np.mean(wac_proximity).round(3)} +- {np.std(wac_proximity).round(3)}\",\n",
    "                f\"{np.mean(wac_plausibility).round(3)} +- {np.std(wac_plausibility).round(3)}\"]]\n",
    "print(tabulate(score_table, headers='firstrow', tablefmt='outline'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2474358c-0c66-444a-b528-d4e99f1d4e78",
   "metadata": {},
   "source": [
    "### Performance Differences\n",
    "\n",
    "**Task 3(e)**: Discuss in your report commenting their performance based on the metrics. Link the findings to their theories."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xaivenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
