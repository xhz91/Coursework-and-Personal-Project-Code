{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import package and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "if str(os.getcwd()).endswith('BertModel'):\n",
    "    os.chdir(\"..\")\n",
    "\n",
    "from BertModel.Analyzer import BertAnalyzer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import f1_score\n",
    "from BertModel.BaselineModels import Bow_Baseline_Model, Tfidf_Baseline_Model\n",
    "import sentencepiece\n",
    "from BertModel.Sampling import DataSampling\n",
    "from BertModel.Analyzer import BertAnalyzer\n",
    "from BertModel.PreTrainedBert import model\n",
    "from BertModel.PreProcessing import BertDataset, generate_batch\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'dontpatronizeme_pcl.tsv'\n",
    "titles = ['par_id', 'art_id', 'keyword','country_code','text','label']\n",
    "raw_data_orig = pd.read_csv(path, skiprows = 4, sep = '\\t',\n",
    "                       names = titles)\n",
    "raw_data = raw_data_orig.fillna(\"missing_value\")\n",
    "raw_data['label'] = np.where(raw_data['label'] > 1, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze class distribution\n",
    "label_counts = raw_data['label'].value_counts()\n",
    "print(label_counts)\n",
    "# Plot class distribution\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.bar(label_counts.index.astype(str), label_counts.values)\n",
    "plt.xticks([0, 1], ['Non-Patronizing', 'Patronizing'])\n",
    "\n",
    "plt.ylabel(\"Number of lines\")\n",
    "plt.title(\"Class Label Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count occurrences of each label per keyword\n",
    "keyword_counts = raw_data.groupby(\"keyword\")[\"label\"].value_counts().unstack()\n",
    "\n",
    "# Plot distribution\n",
    "keyword_counts.plot(kind=\"bar\", stacked=True, figsize=(12,6), colormap=\"viridis\")\n",
    "\n",
    "plt.ylabel(\"Number of lines\")\n",
    "plt.title(\"Distribution of Class Labels for Each Keyword Group\")\n",
    "plt.legend(title=\"Label\", labels=[\"Non-Patronizing\", \"Patronizing\"])\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute proportion of patronizing texts per keyword\n",
    "patronizing_ratio_keyword = raw_data.groupby(\"keyword\")[\"label\"].mean().sort_values(ascending=False)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10,5))\n",
    "patronizing_ratio_keyword.plot(kind=\"bar\", color=\"orange\")\n",
    "\n",
    "plt.ylabel(\"Proportion of Patronizing Texts\")\n",
    "plt.title(\"Keywords with Highest to Lowest Patronizing Language\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count occurrences of each label per country\n",
    "country_label_counts = raw_data.groupby(\"country_code\")[\"label\"].value_counts().unstack()\n",
    "\n",
    "# Plot distribution\n",
    "country_label_counts.plot(kind=\"bar\", stacked=True, figsize=(12,6), colormap=\"viridis\")\n",
    "plt.xlabel(\"Country Code\")\n",
    "plt.ylabel(\"Number of Lines\")\n",
    "plt.title(\"Distribution of Class Labels Across Countries\")\n",
    "plt.legend(title=\"Label\", labels=[\"Non-Patronizing\", \"Patronizing\"])\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute proportion of patronizing texts per country\n",
    "patronizing_ratio = raw_data.groupby(\"country_code\")[\"label\"].mean().sort_values(ascending=False)\n",
    "\n",
    "# Select top 10 countries\n",
    "top_countries = patronizing_ratio.head(10)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10,5))\n",
    "top_countries.plot(kind=\"bar\", color=\"orange\")\n",
    "plt.xlabel(\"Country Code\")\n",
    "plt.ylabel(\"Proportion of Patronizing Texts\")\n",
    "plt.title(\"Top 10 Countries with Highest Patronizing Language\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise text_length distribution, correlation and distribution of labels\n",
    "raw_data[\"text_length\"] = raw_data[\"text\"].apply(lambda x: len(x.split()))\n",
    "bins = [0, 50, 100, 150, 200, float('inf')]\n",
    "bin_names = ['1-50', '51-100', '101-150', '151-200', '>200']\n",
    "raw_data['text_length_bucket'] = pd.cut(raw_data['text_length'], bins=bins, labels=bin_names, right=False)\n",
    "\n",
    "# Step 3: Group by text_length_bucket and label, and count the number of lines\n",
    "grouped = raw_data.groupby(\"text_length_bucket\")[\"label\"].value_counts().unstack()\n",
    "\n",
    "# Step 4: Plot the stacked bar chart\n",
    "grouped.plot(kind=\"bar\", stacked=True, figsize=(6,3), colormap=\"viridis\")\n",
    "plt.xlabel('Text Length Buckets')\n",
    "plt.ylabel('Number of Lines')\n",
    "plt.title('Distribution of Class Labels Across Text Length Buckets')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title=\"Label\", labels=[\"Non-Patronizing\", \"Patronizing\"])\n",
    "plt.show()\n",
    "\n",
    "# Compute proportion of patronizing texts per country\n",
    "patronizing_ratio = raw_data.groupby(\"text_length_bucket\")[\"label\"].mean().sort_values(ascending=False)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10,5))\n",
    "patronizing_ratio.plot(kind=\"bar\", color=\"orange\")\n",
    "plt.xlabel(\"text_length_bucket\")\n",
    "plt.ylabel(\"Proportion of Patronizing Texts\")\n",
    "plt.title(\"Text Length with Highest to Lowest Patronizing Language\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Compute text length and correlation with label\n",
    "correlation = raw_data[[\"text_length\", \"label\"]].corr().iloc[0,1]\n",
    "print(f\"correlation between text_length and label prediction: {correlation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2a - for final model implementation, please check Analyzer.py, please see the code for generating dev.txt and test.txt in this section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train - dev split\n",
    "train = pd.read_csv(\"semeval-2022/practice splits/train_semeval_parids-labels.csv\")\n",
    "dev = pd.read_csv(\"semeval-2022/practice splits/dev_semeval_parids-labels.csv\")\n",
    "train_df = raw_data[raw_data[\"par_id\"].isin(train['par_id'])]\n",
    "dev_df = raw_data[raw_data[\"par_id\"].isin(dev['par_id'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1:2 ratio between positive and negative\n",
    "def downsample(raw_data, seeds = 42):\n",
    "    # downsampling the unpatronizing text data based on the keyword\n",
    "    keywords = raw_data['keyword'].unique()\n",
    "    dfs = []\n",
    "    for keyword in keywords:\n",
    "        patro_df = raw_data[(raw_data['keyword'] == keyword) & (raw_data['label'] == 1)]\n",
    "        non_patro_df = raw_data[(raw_data['keyword'] == keyword) & (raw_data['label'] == 0)]\n",
    "        patro_count = len(patro_df)\n",
    "        select_patro_df = non_patro_df.sample(n=2*patro_count, random_state=seeds)\n",
    "        downsampled_df = pd.concat([patro_df, select_patro_df])\n",
    "        dfs.append( downsampled_df)\n",
    "    result = pd.concat(dfs)\n",
    "    return result.sample(frac = 1, random_state=seeds)\n",
    "\n",
    "train_df = downsample(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlnet_model = model(\"xlnet-base-cased\")\n",
    "xlnet_analyzer = BertAnalyzer(model = xlnet_model,\n",
    "                                batch_size=64,\n",
    "                                max_seq_len=128,\n",
    "                                epochs=5,\n",
    "                                lr=4e-5)\n",
    "xlnet_analyzer.train(train_df, None)\n",
    "\n",
    "xlnetmodel = xlnet_analyzer.net\n",
    "dev_data = BertDataset.from_data(dev_df)\n",
    "test_loader = DataLoader(dev_data,\n",
    "                         batch_size = 64,\n",
    "                         shuffle = False, \n",
    "                         num_workers = 4,\n",
    "                         collate_fn = lambda batch: generate_batch(batch, max_seq_len = 128))\n",
    "predicted = []\n",
    "truths = []\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        print(1)\n",
    "        input_ids, attn_mask, labels = tuple(i.to(device) for i in batch)\n",
    "        outputs = xlnetmodel(input_ids, attn_mask).squeeze(dim = 1)\n",
    "        pred = (outputs >=0).int()\n",
    "        predicted += pred.tolist()\n",
    "        truths += labels.tolist()\n",
    "with open(\"dev.txt\", 'w') as file:\n",
    "    for number in predicted:\n",
    "        file.write(str(number) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"dev.txt\", \"r\") as file:\n",
    "    dev = [int(line.strip()) for line in file]\n",
    "print(len(dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "f1 = metrics.f1_score(dev_df[\"label\"].tolist(), dev)\n",
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'semeval-2022/TEST/task4_test.tsv'\n",
    "titles = ['par_id', 'art_id', 'keyword','country_code','text','label']\n",
    "test_df = pd.read_csv(path, sep = '\\t',\n",
    "                       names = titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = BertDataset.from_data(test_df)\n",
    "test_loader = DataLoader(test_data,\n",
    "                         batch_size = 64,\n",
    "                         shuffle = False, \n",
    "                         num_workers = 4,\n",
    "                         collate_fn = lambda batch: generate_batch(batch, max_seq_len = 128))\n",
    "predicted = []\n",
    "truths = []\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids, attn_mask, labels = tuple(i.to(device) for i in batch)\n",
    "        outputs = xlnetmodel(input_ids, attn_mask).squeeze(dim = 1)\n",
    "        pred = (outputs >=0).int()\n",
    "        predicted += pred.tolist()\n",
    "        truths += labels.tolist()\n",
    "with open(\"test.txt\", 'w') as file:\n",
    "    for number in predicted:\n",
    "        file.write(str(number) + '\\n')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2b Hyperparameter Tuning, please see below, and Scheduler.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train - validation (dev) - test split\n",
    "train = pd.read_csv(\"semeval-2022/practice splits/train_semeval_parids-labels.csv\")\n",
    "test = pd.read_csv(\"semeval-2022/practice splits/dev_semeval_parids-labels.csv\")\n",
    "train_df_official = raw_data[raw_data[\"par_id\"].isin(train['par_id'])]\n",
    "test_df = raw_data[raw_data[\"par_id\"].isin(test['par_id'])]\n",
    "\n",
    "train_data_shuffled = train_df_official.sample(frac = 1, random_state = 1).reset_index(drop = True)\n",
    "split_index = int(0.8 * len(train_data_shuffled))\n",
    "\n",
    "train_df = train_data_shuffled.iloc[:split_index]\n",
    "val_df = train_data_shuffled.iloc[split_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "xlnet_model = model(\"xlnet-base-cased\")\n",
    "best_f1 = 0\n",
    "best_params = {}\n",
    "all_params = {}\n",
    "\n",
    "learning_rates = [1e-4, 4e-5, 1e-5]  # Standard range for BERT fine-tuning\n",
    "batch_sizes = [32, 64, 128]  # Adjust based on GPU memory\n",
    "max_token_lens = [64, 128]  # Typical for BERT fine-tuning\n",
    "\n",
    "# Grid Search Loop\n",
    "for lr, batch_size, max_token_len in itertools.product(learning_rates, batch_sizes, max_token_lens):\n",
    "    xlnet_model = model(\"xlnet-base-cased\")\n",
    "    if batch_size == 128 and max_token_len == 128:\n",
    "        continue\n",
    "    print(f\"\\nTraining with: LR={lr}, Batch Size={batch_size}, max_token_len={max_token_len}\")\n",
    "    \n",
    "    # Define model arguments\n",
    "    xlnet_analyzer = BertAnalyzer(model = xlnet_model,\n",
    "                                    batch_size=batch_size,\n",
    "                                    max_seq_len=max_token_len,\n",
    "                                    epochs=3,\n",
    "                                    lr=lr)\n",
    "    \n",
    "    datasampling = DataSampling()\n",
    "    data = datasampling.downsample(train_df)\n",
    "\n",
    "    xlnet_analyzer.train(data)\n",
    "    f1 = xlnet_analyzer.evaluate(val_df)\n",
    "\n",
    "    all_params[(lr, batch_size, max_token_len)] = f1\n",
    "    del xlnet_model\n",
    "    del xlnet_analyzer\n",
    "    # Check if this is the best model so far\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_params = {\"learning_rate\": lr, \"batch_size\": batch_size, \"max_token_len\": max_token_len}\n",
    "\n",
    "    print(f\"F1-score: {f1:.4f}\")\n",
    "\n",
    "# Print best hyperparameters\n",
    "print(\"\\n Best Hyperparameters:\")\n",
    "print(best_params)\n",
    "print(f\"Best F1-score:Â {best_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2c: for sampling and augmentation, please check Sampling.py, and upsample_ratio_test.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2d, please see below, and BaselineModels.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train - validation (dev) - test split\n",
    "train = pd.read_csv(\"semeval-2022/practice splits/train_semeval_parids-labels.csv\")\n",
    "test = pd.read_csv(\"semeval-2022/practice splits/dev_semeval_parids-labels.csv\")\n",
    "train_df_official = raw_data[raw_data[\"par_id\"].isin(train['par_id'])]\n",
    "test_df = raw_data[raw_data[\"par_id\"].isin(test['par_id'])]\n",
    "\n",
    "train_data_shuffled = train_df_official.sample(frac = 1, random_state = 1).reset_index(drop = True)\n",
    "split_index = int(0.8 * len(train_data_shuffled))\n",
    "\n",
    "train_df = train_data_shuffled.iloc[:split_index]\n",
    "val_df = train_data_shuffled.iloc[split_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train BoW model\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "print(\"Training BoW model...\")\n",
    "model_bow = Bow_Baseline_Model()\n",
    "model_bow.train(train_df)\n",
    "print(\"Testing BoW model...\")\n",
    "f1_scores_bow = model_bow.test(test_df)\n",
    "\n",
    "# Initialize and train TF-IDF model\n",
    "print(\"\\nTraining TF-IDF model...\")\n",
    "model_tfidf = Tfidf_Baseline_Model()\n",
    "model_tfidf.train(train_df)\n",
    "print(\"Testing TF-IDF model...\")\n",
    "f1_scores_tfidf = model_tfidf.test(test_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"semeval-2022/practice splits/train_semeval_parids-labels.csv\")\n",
    "test = pd.read_csv(\"semeval-2022/practice splits/dev_semeval_parids-labels.csv\")\n",
    "train_df = raw_data[raw_data[\"par_id\"].isin(train['par_id'])]\n",
    "test_df = raw_data[raw_data[\"par_id\"].isin(test['par_id'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasampling = DataSampling()\n",
    "data = datasampling.downsample(train_df)\n",
    "xlnet_model = model(\"xlnet-base-cased\")\n",
    "xlnet_analyzer = BertAnalyzer(model=xlnet_model,\n",
    "                              batch_size=64,\n",
    "                              max_seq_len=128,\n",
    "                              epochs=3,\n",
    "                              lr=4e-05)\n",
    "save_dir = \"xlnet_analyzer_train_save\"\n",
    "save_path = os.path.join(save_dir, f\"part3.pth\")\n",
    "xlnet_analyzer.train(data, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score = xlnet_analyzer.evaluate(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part 3a\n",
    "raw_data_orig = raw_data_orig.dropna()\n",
    "test_str_df = raw_data_orig[raw_data_orig[\"par_id\"].isin(test['par_id'])]\n",
    "test_df['original'] = test_str_df['label']\n",
    "for original_label in test_df['original'].unique():\n",
    "    original_df = test_df[(test_df['original'] == original_label)]\n",
    "    if len(original_df) > 0:\n",
    "        print(f\"Original label {original_label}\")\n",
    "        f1_score = xlnet_analyzer.evaluate(original_df) #note: evaluate() method will print out accuracy and f1, as defined in Analyzer\n",
    "        count = len(test_df[(test_df['original'] == original_label)])\n",
    "        print(f\"original_label {original_label}: {count} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part 3b - input length\n",
    "# Function to compute text length and bucketize\n",
    "def get_length_buckets(texts, bucket_size=20):\n",
    "    lengths = texts.str.split().apply(len)\n",
    "    bins = np.arange(0, lengths.max() + bucket_size, bucket_size)\n",
    "    bucket_labels = [f\"{b}-{b+bucket_size}\" for b in bins[:-1]]\n",
    "    length_buckets = pd.cut(lengths, bins=bins, labels=bucket_labels, right=False)\n",
    "    return lengths, length_buckets\n",
    "\n",
    "# Get input lengths and bucket them\n",
    "test_df[\"length\"], test_df[\"length_bucket\"] = get_length_buckets(test_df[\"text\"])\n",
    "\n",
    "# Compute performance metrics per length bucket\n",
    "for bucket in test_df[\"length_bucket\"].unique():\n",
    "    subset = test_df[test_df[\"length_bucket\"] == bucket]\n",
    "    if len(subset) > 0:\n",
    "        print(f\"Input length bucket {bucket}\")\n",
    "        f1_score = xlnet_analyzer.evaluate(subset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part 3c - data categories\n",
    "for keyword in test_df['keyword'].unique():\n",
    "    keyword_df = test_df[(test_df['keyword'] == keyword) ]\n",
    "    if len(keyword_df) > 0:\n",
    "        print(f\"Keyword {keyword}\")\n",
    "        f1_score = xlnet_analyzer.evaluate(keyword_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
